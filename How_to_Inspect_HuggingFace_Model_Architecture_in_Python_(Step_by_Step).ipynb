{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# How to Inspect HuggingFace Model Architecture in Python (Step-by-Step)"
      ],
      "metadata": {
        "id": "psTUbA-wrSBi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading the model"
      ],
      "metadata": {
        "id": "pI32ZLtFq2Uo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Galqzj6_nizx"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "device = \"cuda\"\n",
        "model_path = \"ibm-granite/granite-4.0-h-350M\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "# drop device_map if running on CPU\n",
        "model = AutoModelForCausalLM.from_pretrained(model_path, device_map=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Method 1 â€” Print the model structure"
      ],
      "metadata": {
        "id": "BG2yOwclq5JG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "2znygqKHokH-",
        "outputId": "2e539a0e-e3d5-4461-a75a-33d8956e8202"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GraniteMoeHybridForCausalLM(\n",
            "  (model): GraniteMoeHybridModel(\n",
            "    (embed_tokens): Embedding(100352, 768, padding_idx=100256)\n",
            "    (layers): ModuleList(\n",
            "      (0-9): 10 x GraniteMoeHybridDecoderLayer(\n",
            "        (input_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "        (post_attention_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "        (shared_mlp): GraniteMoeHybridMLP(\n",
            "          (activation): SiLUActivation()\n",
            "          (input_linear): Linear(in_features=768, out_features=4096, bias=False)\n",
            "          (output_linear): Linear(in_features=2048, out_features=768, bias=False)\n",
            "        )\n",
            "        (mamba): GraniteMoeHybridMambaLayer(\n",
            "          (act): SiLUActivation()\n",
            "          (conv1d): Conv1d(1792, 1792, kernel_size=(4,), stride=(1,), padding=(3,), groups=1792)\n",
            "          (in_proj): Linear(in_features=768, out_features=3376, bias=False)\n",
            "          (norm): GraniteMoeHybridRMSNormGated()\n",
            "          (out_proj): Linear(in_features=1536, out_features=768, bias=False)\n",
            "        )\n",
            "      )\n",
            "      (10): GraniteMoeHybridDecoderLayer(\n",
            "        (input_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "        (post_attention_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "        (shared_mlp): GraniteMoeHybridMLP(\n",
            "          (activation): SiLUActivation()\n",
            "          (input_linear): Linear(in_features=768, out_features=4096, bias=False)\n",
            "          (output_linear): Linear(in_features=2048, out_features=768, bias=False)\n",
            "        )\n",
            "        (self_attn): GraniteMoeHybridAttention(\n",
            "          (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
            "          (k_proj): Linear(in_features=768, out_features=256, bias=False)\n",
            "          (v_proj): Linear(in_features=768, out_features=256, bias=False)\n",
            "          (o_proj): Linear(in_features=768, out_features=768, bias=False)\n",
            "        )\n",
            "      )\n",
            "      (11-12): 2 x GraniteMoeHybridDecoderLayer(\n",
            "        (input_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "        (post_attention_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "        (shared_mlp): GraniteMoeHybridMLP(\n",
            "          (activation): SiLUActivation()\n",
            "          (input_linear): Linear(in_features=768, out_features=4096, bias=False)\n",
            "          (output_linear): Linear(in_features=2048, out_features=768, bias=False)\n",
            "        )\n",
            "        (mamba): GraniteMoeHybridMambaLayer(\n",
            "          (act): SiLUActivation()\n",
            "          (conv1d): Conv1d(1792, 1792, kernel_size=(4,), stride=(1,), padding=(3,), groups=1792)\n",
            "          (in_proj): Linear(in_features=768, out_features=3376, bias=False)\n",
            "          (norm): GraniteMoeHybridRMSNormGated()\n",
            "          (out_proj): Linear(in_features=1536, out_features=768, bias=False)\n",
            "        )\n",
            "      )\n",
            "      (13): GraniteMoeHybridDecoderLayer(\n",
            "        (input_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "        (post_attention_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "        (shared_mlp): GraniteMoeHybridMLP(\n",
            "          (activation): SiLUActivation()\n",
            "          (input_linear): Linear(in_features=768, out_features=4096, bias=False)\n",
            "          (output_linear): Linear(in_features=2048, out_features=768, bias=False)\n",
            "        )\n",
            "        (self_attn): GraniteMoeHybridAttention(\n",
            "          (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
            "          (k_proj): Linear(in_features=768, out_features=256, bias=False)\n",
            "          (v_proj): Linear(in_features=768, out_features=256, bias=False)\n",
            "          (o_proj): Linear(in_features=768, out_features=768, bias=False)\n",
            "        )\n",
            "      )\n",
            "      (14-16): 3 x GraniteMoeHybridDecoderLayer(\n",
            "        (input_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "        (post_attention_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "        (shared_mlp): GraniteMoeHybridMLP(\n",
            "          (activation): SiLUActivation()\n",
            "          (input_linear): Linear(in_features=768, out_features=4096, bias=False)\n",
            "          (output_linear): Linear(in_features=2048, out_features=768, bias=False)\n",
            "        )\n",
            "        (mamba): GraniteMoeHybridMambaLayer(\n",
            "          (act): SiLUActivation()\n",
            "          (conv1d): Conv1d(1792, 1792, kernel_size=(4,), stride=(1,), padding=(3,), groups=1792)\n",
            "          (in_proj): Linear(in_features=768, out_features=3376, bias=False)\n",
            "          (norm): GraniteMoeHybridRMSNormGated()\n",
            "          (out_proj): Linear(in_features=1536, out_features=768, bias=False)\n",
            "        )\n",
            "      )\n",
            "      (17): GraniteMoeHybridDecoderLayer(\n",
            "        (input_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "        (post_attention_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "        (shared_mlp): GraniteMoeHybridMLP(\n",
            "          (activation): SiLUActivation()\n",
            "          (input_linear): Linear(in_features=768, out_features=4096, bias=False)\n",
            "          (output_linear): Linear(in_features=2048, out_features=768, bias=False)\n",
            "        )\n",
            "        (self_attn): GraniteMoeHybridAttention(\n",
            "          (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
            "          (k_proj): Linear(in_features=768, out_features=256, bias=False)\n",
            "          (v_proj): Linear(in_features=768, out_features=256, bias=False)\n",
            "          (o_proj): Linear(in_features=768, out_features=768, bias=False)\n",
            "        )\n",
            "      )\n",
            "      (18-26): 9 x GraniteMoeHybridDecoderLayer(\n",
            "        (input_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "        (post_attention_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "        (shared_mlp): GraniteMoeHybridMLP(\n",
            "          (activation): SiLUActivation()\n",
            "          (input_linear): Linear(in_features=768, out_features=4096, bias=False)\n",
            "          (output_linear): Linear(in_features=2048, out_features=768, bias=False)\n",
            "        )\n",
            "        (mamba): GraniteMoeHybridMambaLayer(\n",
            "          (act): SiLUActivation()\n",
            "          (conv1d): Conv1d(1792, 1792, kernel_size=(4,), stride=(1,), padding=(3,), groups=1792)\n",
            "          (in_proj): Linear(in_features=768, out_features=3376, bias=False)\n",
            "          (norm): GraniteMoeHybridRMSNormGated()\n",
            "          (out_proj): Linear(in_features=1536, out_features=768, bias=False)\n",
            "        )\n",
            "      )\n",
            "      (27): GraniteMoeHybridDecoderLayer(\n",
            "        (input_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "        (post_attention_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "        (shared_mlp): GraniteMoeHybridMLP(\n",
            "          (activation): SiLUActivation()\n",
            "          (input_linear): Linear(in_features=768, out_features=4096, bias=False)\n",
            "          (output_linear): Linear(in_features=2048, out_features=768, bias=False)\n",
            "        )\n",
            "        (self_attn): GraniteMoeHybridAttention(\n",
            "          (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
            "          (k_proj): Linear(in_features=768, out_features=256, bias=False)\n",
            "          (v_proj): Linear(in_features=768, out_features=256, bias=False)\n",
            "          (o_proj): Linear(in_features=768, out_features=768, bias=False)\n",
            "        )\n",
            "      )\n",
            "      (28-31): 4 x GraniteMoeHybridDecoderLayer(\n",
            "        (input_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "        (post_attention_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "        (shared_mlp): GraniteMoeHybridMLP(\n",
            "          (activation): SiLUActivation()\n",
            "          (input_linear): Linear(in_features=768, out_features=4096, bias=False)\n",
            "          (output_linear): Linear(in_features=2048, out_features=768, bias=False)\n",
            "        )\n",
            "        (mamba): GraniteMoeHybridMambaLayer(\n",
            "          (act): SiLUActivation()\n",
            "          (conv1d): Conv1d(1792, 1792, kernel_size=(4,), stride=(1,), padding=(3,), groups=1792)\n",
            "          (in_proj): Linear(in_features=768, out_features=3376, bias=False)\n",
            "          (norm): GraniteMoeHybridRMSNormGated()\n",
            "          (out_proj): Linear(in_features=1536, out_features=768, bias=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (norm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "  )\n",
            "  (lm_head): Linear(in_features=768, out_features=100352, bias=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Method 2 â€” Use module introspection & per-layer inspection"
      ],
      "metadata": {
        "id": "svOAigIEq7uc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for name, module in model.named_modules():\n",
        "    print(name, \"->\", module)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "M6F9Z5y7oD0K",
        "outputId": "c9ed48ac-0d1d-4e98-d588-5051019a4454"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " -> GraniteMoeHybridForCausalLM(\n",
            "  (model): GraniteMoeHybridModel(\n",
            "    (embed_tokens): Embedding(100352, 768, padding_idx=100256)\n",
            "    (layers): ModuleList(\n",
            "      (0-9): 10 x GraniteMoeHybridDecoderLayer(\n",
            "        (input_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "        (post_attention_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "        (shared_mlp): GraniteMoeHybridMLP(\n",
            "          (activation): SiLUActivation()\n",
            "          (input_linear): Linear(in_features=768, out_features=4096, bias=False)\n",
            "          (output_linear): Linear(in_features=2048, out_features=768, bias=False)\n",
            "        )\n",
            "        (mamba): GraniteMoeHybridMambaLayer(\n",
            "          (act): SiLUActivation()\n",
            "          (conv1d): Conv1d(1792, 1792, kernel_size=(4,), stride=(1,), padding=(3,), groups=1792)\n",
            "          (in_proj): Linear(in_features=768, out_features=3376, bias=False)\n",
            "          (norm): GraniteMoeHybridRMSNormGated()\n",
            "          (out_proj): Linear(in_features=1536, out_features=768, bias=False)\n",
            "        )\n",
            "      )\n",
            "      (10): GraniteMoeHybridDecoderLayer(\n",
            "        (input_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "        (post_attention_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "        (shared_mlp): GraniteMoeHybridMLP(\n",
            "          (activation): SiLUActivation()\n",
            "          (input_linear): Linear(in_features=768, out_features=4096, bias=False)\n",
            "          (output_linear): Linear(in_features=2048, out_features=768, bias=False)\n",
            "        )\n",
            "        (self_attn): GraniteMoeHybridAttention(\n",
            "          (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
            "          (k_proj): Linear(in_features=768, out_features=256, bias=False)\n",
            "          (v_proj): Linear(in_features=768, out_features=256, bias=False)\n",
            "          (o_proj): Linear(in_features=768, out_features=768, bias=False)\n",
            "        )\n",
            "      )\n",
            "      (11-12): 2 x GraniteMoeHybridDecoderLayer(\n",
            "        (input_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "        (post_attention_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "        (shared_mlp): GraniteMoeHybridMLP(\n",
            "          (activation): SiLUActivation()\n",
            "          (input_linear): Linear(in_features=768, out_features=4096, bias=False)\n",
            "          (output_linear): Linear(in_features=2048, out_features=768, bias=False)\n",
            "        )\n",
            "        (mamba): GraniteMoeHybridMambaLayer(\n",
            "          (act): SiLUActivation()\n",
            "          (conv1d): Conv1d(1792, 1792, kernel_size=(4,), stride=(1,), padding=(3,), groups=1792)\n",
            "          (in_proj): Linear(in_features=768, out_features=3376, bias=False)\n",
            "          (norm): GraniteMoeHybridRMSNormGated()\n",
            "          (out_proj): Linear(in_features=1536, out_features=768, bias=False)\n",
            "        )\n",
            "      )\n",
            "      (13): GraniteMoeHybridDecoderLayer(\n",
            "        (input_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "        (post_attention_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "        (shared_mlp): GraniteMoeHybridMLP(\n",
            "          (activation): SiLUActivation()\n",
            "          (input_linear): Linear(in_features=768, out_features=4096, bias=False)\n",
            "          (output_linear): Linear(in_features=2048, out_features=768, bias=False)\n",
            "        )\n",
            "        (self_attn): GraniteMoeHybridAttention(\n",
            "          (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
            "          (k_proj): Linear(in_features=768, out_features=256, bias=False)\n",
            "          (v_proj): Linear(in_features=768, out_features=256, bias=False)\n",
            "          (o_proj): Linear(in_features=768, out_features=768, bias=False)\n",
            "        )\n",
            "      )\n",
            "      (14-16): 3 x GraniteMoeHybridDecoderLayer(\n",
            "        (input_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "        (post_attention_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "        (shared_mlp): GraniteMoeHybridMLP(\n",
            "          (activation): SiLUActivation()\n",
            "          (input_linear): Linear(in_features=768, out_features=4096, bias=False)\n",
            "          (output_linear): Linear(in_features=2048, out_features=768, bias=False)\n",
            "        )\n",
            "        (mamba): GraniteMoeHybridMambaLayer(\n",
            "          (act): SiLUActivation()\n",
            "          (conv1d): Conv1d(1792, 1792, kernel_size=(4,), stride=(1,), padding=(3,), groups=1792)\n",
            "          (in_proj): Linear(in_features=768, out_features=3376, bias=False)\n",
            "          (norm): GraniteMoeHybridRMSNormGated()\n",
            "          (out_proj): Linear(in_features=1536, out_features=768, bias=False)\n",
            "        )\n",
            "      )\n",
            "      (17): GraniteMoeHybridDecoderLayer(\n",
            "        (input_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "        (post_attention_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "        (shared_mlp): GraniteMoeHybridMLP(\n",
            "          (activation): SiLUActivation()\n",
            "          (input_linear): Linear(in_features=768, out_features=4096, bias=False)\n",
            "          (output_linear): Linear(in_features=2048, out_features=768, bias=False)\n",
            "        )\n",
            "        (self_attn): GraniteMoeHybridAttention(\n",
            "          (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
            "          (k_proj): Linear(in_features=768, out_features=256, bias=False)\n",
            "          (v_proj): Linear(in_features=768, out_features=256, bias=False)\n",
            "          (o_proj): Linear(in_features=768, out_features=768, bias=False)\n",
            "        )\n",
            "      )\n",
            "      (18-26): 9 x GraniteMoeHybridDecoderLayer(\n",
            "        (input_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "        (post_attention_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "        (shared_mlp): GraniteMoeHybridMLP(\n",
            "          (activation): SiLUActivation()\n",
            "          (input_linear): Linear(in_features=768, out_features=4096, bias=False)\n",
            "          (output_linear): Linear(in_features=2048, out_features=768, bias=False)\n",
            "        )\n",
            "        (mamba): GraniteMoeHybridMambaLayer(\n",
            "          (act): SiLUActivation()\n",
            "          (conv1d): Conv1d(1792, 1792, kernel_size=(4,), stride=(1,), padding=(3,), groups=1792)\n",
            "          (in_proj): Linear(in_features=768, out_features=3376, bias=False)\n",
            "          (norm): GraniteMoeHybridRMSNormGated()\n",
            "          (out_proj): Linear(in_features=1536, out_features=768, bias=False)\n",
            "        )\n",
            "      )\n",
            "      (27): GraniteMoeHybridDecoderLayer(\n",
            "        (input_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "        (post_attention_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "        (shared_mlp): GraniteMoeHybridMLP(\n",
            "          (activation): SiLUActivation()\n",
            "          (input_linear): Linear(in_features=768, out_features=4096, bias=False)\n",
            "          (output_linear): Linear(in_features=2048, out_features=768, bias=False)\n",
            "        )\n",
            "        (self_attn): GraniteMoeHybridAttention(\n",
            "          (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
            "          (k_proj): Linear(in_features=768, out_features=256, bias=False)\n",
            "          (v_proj): Linear(in_features=768, out_features=256, bias=False)\n",
            "          (o_proj): Linear(in_features=768, out_features=768, bias=False)\n",
            "        )\n",
            "      )\n",
            "      (28-31): 4 x GraniteMoeHybridDecoderLayer(\n",
            "        (input_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "        (post_attention_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "        (shared_mlp): GraniteMoeHybridMLP(\n",
            "          (activation): SiLUActivation()\n",
            "          (input_linear): Linear(in_features=768, out_features=4096, bias=False)\n",
            "          (output_linear): Linear(in_features=2048, out_features=768, bias=False)\n",
            "        )\n",
            "        (mamba): GraniteMoeHybridMambaLayer(\n",
            "          (act): SiLUActivation()\n",
            "          (conv1d): Conv1d(1792, 1792, kernel_size=(4,), stride=(1,), padding=(3,), groups=1792)\n",
            "          (in_proj): Linear(in_features=768, out_features=3376, bias=False)\n",
            "          (norm): GraniteMoeHybridRMSNormGated()\n",
            "          (out_proj): Linear(in_features=1536, out_features=768, bias=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (norm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "  )\n",
            "  (lm_head): Linear(in_features=768, out_features=100352, bias=False)\n",
            ")\n",
            "model -> GraniteMoeHybridModel(\n",
            "  (embed_tokens): Embedding(100352, 768, padding_idx=100256)\n",
            "  (layers): ModuleList(\n",
            "    (0-9): 10 x GraniteMoeHybridDecoderLayer(\n",
            "      (input_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "      (post_attention_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "      (shared_mlp): GraniteMoeHybridMLP(\n",
            "        (activation): SiLUActivation()\n",
            "        (input_linear): Linear(in_features=768, out_features=4096, bias=False)\n",
            "        (output_linear): Linear(in_features=2048, out_features=768, bias=False)\n",
            "      )\n",
            "      (mamba): GraniteMoeHybridMambaLayer(\n",
            "        (act): SiLUActivation()\n",
            "        (conv1d): Conv1d(1792, 1792, kernel_size=(4,), stride=(1,), padding=(3,), groups=1792)\n",
            "        (in_proj): Linear(in_features=768, out_features=3376, bias=False)\n",
            "        (norm): GraniteMoeHybridRMSNormGated()\n",
            "        (out_proj): Linear(in_features=1536, out_features=768, bias=False)\n",
            "      )\n",
            "    )\n",
            "    (10): GraniteMoeHybridDecoderLayer(\n",
            "      (input_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "      (post_attention_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "      (shared_mlp): GraniteMoeHybridMLP(\n",
            "        (activation): SiLUActivation()\n",
            "        (input_linear): Linear(in_features=768, out_features=4096, bias=False)\n",
            "        (output_linear): Linear(in_features=2048, out_features=768, bias=False)\n",
            "      )\n",
            "      (self_attn): GraniteMoeHybridAttention(\n",
            "        (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (k_proj): Linear(in_features=768, out_features=256, bias=False)\n",
            "        (v_proj): Linear(in_features=768, out_features=256, bias=False)\n",
            "        (o_proj): Linear(in_features=768, out_features=768, bias=False)\n",
            "      )\n",
            "    )\n",
            "    (11-12): 2 x GraniteMoeHybridDecoderLayer(\n",
            "      (input_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "      (post_attention_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "      (shared_mlp): GraniteMoeHybridMLP(\n",
            "        (activation): SiLUActivation()\n",
            "        (input_linear): Linear(in_features=768, out_features=4096, bias=False)\n",
            "        (output_linear): Linear(in_features=2048, out_features=768, bias=False)\n",
            "      )\n",
            "      (mamba): GraniteMoeHybridMambaLayer(\n",
            "        (act): SiLUActivation()\n",
            "        (conv1d): Conv1d(1792, 1792, kernel_size=(4,), stride=(1,), padding=(3,), groups=1792)\n",
            "        (in_proj): Linear(in_features=768, out_features=3376, bias=False)\n",
            "        (norm): GraniteMoeHybridRMSNormGated()\n",
            "        (out_proj): Linear(in_features=1536, out_features=768, bias=False)\n",
            "      )\n",
            "    )\n",
            "    (13): GraniteMoeHybridDecoderLayer(\n",
            "      (input_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "      (post_attention_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "      (shared_mlp): GraniteMoeHybridMLP(\n",
            "        (activation): SiLUActivation()\n",
            "        (input_linear): Linear(in_features=768, out_features=4096, bias=False)\n",
            "        (output_linear): Linear(in_features=2048, out_features=768, bias=False)\n",
            "      )\n",
            "      (self_attn): GraniteMoeHybridAttention(\n",
            "        (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (k_proj): Linear(in_features=768, out_features=256, bias=False)\n",
            "        (v_proj): Linear(in_features=768, out_features=256, bias=False)\n",
            "        (o_proj): Linear(in_features=768, out_features=768, bias=False)\n",
            "      )\n",
            "    )\n",
            "    (14-16): 3 x GraniteMoeHybridDecoderLayer(\n",
            "      (input_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "      (post_attention_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "      (shared_mlp): GraniteMoeHybridMLP(\n",
            "        (activation): SiLUActivation()\n",
            "        (input_linear): Linear(in_features=768, out_features=4096, bias=False)\n",
            "        (output_linear): Linear(in_features=2048, out_features=768, bias=False)\n",
            "      )\n",
            "      (mamba): GraniteMoeHybridMambaLayer(\n",
            "        (act): SiLUActivation()\n",
            "        (conv1d): Conv1d(1792, 1792, kernel_size=(4,), stride=(1,), padding=(3,), groups=1792)\n",
            "        (in_proj): Linear(in_features=768, out_features=3376, bias=False)\n",
            "        (norm): GraniteMoeHybridRMSNormGated()\n",
            "        (out_proj): Linear(in_features=1536, out_features=768, bias=False)\n",
            "      )\n",
            "    )\n",
            "    (17): GraniteMoeHybridDecoderLayer(\n",
            "      (input_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "      (post_attention_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "      (shared_mlp): GraniteMoeHybridMLP(\n",
            "        (activation): SiLUActivation()\n",
            "        (input_linear): Linear(in_features=768, out_features=4096, bias=False)\n",
            "        (output_linear): Linear(in_features=2048, out_features=768, bias=False)\n",
            "      )\n",
            "      (self_attn): GraniteMoeHybridAttention(\n",
            "        (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (k_proj): Linear(in_features=768, out_features=256, bias=False)\n",
            "        (v_proj): Linear(in_features=768, out_features=256, bias=False)\n",
            "        (o_proj): Linear(in_features=768, out_features=768, bias=False)\n",
            "      )\n",
            "    )\n",
            "    (18-26): 9 x GraniteMoeHybridDecoderLayer(\n",
            "      (input_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "      (post_attention_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "      (shared_mlp): GraniteMoeHybridMLP(\n",
            "        (activation): SiLUActivation()\n",
            "        (input_linear): Linear(in_features=768, out_features=4096, bias=False)\n",
            "        (output_linear): Linear(in_features=2048, out_features=768, bias=False)\n",
            "      )\n",
            "      (mamba): GraniteMoeHybridMambaLayer(\n",
            "        (act): SiLUActivation()\n",
            "        (conv1d): Conv1d(1792, 1792, kernel_size=(4,), stride=(1,), padding=(3,), groups=1792)\n",
            "        (in_proj): Linear(in_features=768, out_features=3376, bias=False)\n",
            "        (norm): GraniteMoeHybridRMSNormGated()\n",
            "        (out_proj): Linear(in_features=1536, out_features=768, bias=False)\n",
            "      )\n",
            "    )\n",
            "    (27): GraniteMoeHybridDecoderLayer(\n",
            "      (input_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "      (post_attention_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "      (shared_mlp): GraniteMoeHybridMLP(\n",
            "        (activation): SiLUActivation()\n",
            "        (input_linear): Linear(in_features=768, out_features=4096, bias=False)\n",
            "        (output_linear): Linear(in_features=2048, out_features=768, bias=False)\n",
            "      )\n",
            "      (self_attn): GraniteMoeHybridAttention(\n",
            "        (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (k_proj): Linear(in_features=768, out_features=256, bias=False)\n",
            "        (v_proj): Linear(in_features=768, out_features=256, bias=False)\n",
            "        (o_proj): Linear(in_features=768, out_features=768, bias=False)\n",
            "      )\n",
            "    )\n",
            "    (28-31): 4 x GraniteMoeHybridDecoderLayer(\n",
            "      (input_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "      (post_attention_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "      (shared_mlp): GraniteMoeHybridMLP(\n",
            "        (activation): SiLUActivation()\n",
            "        (input_linear): Linear(in_features=768, out_features=4096, bias=False)\n",
            "        (output_linear): Linear(in_features=2048, out_features=768, bias=False)\n",
            "      )\n",
            "      (mamba): GraniteMoeHybridMambaLayer(\n",
            "        (act): SiLUActivation()\n",
            "        (conv1d): Conv1d(1792, 1792, kernel_size=(4,), stride=(1,), padding=(3,), groups=1792)\n",
            "        (in_proj): Linear(in_features=768, out_features=3376, bias=False)\n",
            "        (norm): GraniteMoeHybridRMSNormGated()\n",
            "        (out_proj): Linear(in_features=1536, out_features=768, bias=False)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (norm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            ")\n",
            "model.embed_tokens -> Embedding(100352, 768, padding_idx=100256)\n",
            "model.layers -> ModuleList(\n",
            "  (0-9): 10 x GraniteMoeHybridDecoderLayer(\n",
            "    (input_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "    (post_attention_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "    (shared_mlp): GraniteMoeHybridMLP(\n",
            "      (activation): SiLUActivation()\n",
            "      (input_linear): Linear(in_features=768, out_features=4096, bias=False)\n",
            "      (output_linear): Linear(in_features=2048, out_features=768, bias=False)\n",
            "    )\n",
            "    (mamba): GraniteMoeHybridMambaLayer(\n",
            "      (act): SiLUActivation()\n",
            "      (conv1d): Conv1d(1792, 1792, kernel_size=(4,), stride=(1,), padding=(3,), groups=1792)\n",
            "      (in_proj): Linear(in_features=768, out_features=3376, bias=False)\n",
            "      (norm): GraniteMoeHybridRMSNormGated()\n",
            "      (out_proj): Linear(in_features=1536, out_features=768, bias=False)\n",
            "    )\n",
            "  )\n",
            "  (10): GraniteMoeHybridDecoderLayer(\n",
            "    (input_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "    (post_attention_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "    (shared_mlp): GraniteMoeHybridMLP(\n",
            "      (activation): SiLUActivation()\n",
            "      (input_linear): Linear(in_features=768, out_features=4096, bias=False)\n",
            "      (output_linear): Linear(in_features=2048, out_features=768, bias=False)\n",
            "    )\n",
            "    (self_attn): GraniteMoeHybridAttention(\n",
            "      (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
            "      (k_proj): Linear(in_features=768, out_features=256, bias=False)\n",
            "      (v_proj): Linear(in_features=768, out_features=256, bias=False)\n",
            "      (o_proj): Linear(in_features=768, out_features=768, bias=False)\n",
            "    )\n",
            "  )\n",
            "  (11-12): 2 x GraniteMoeHybridDecoderLayer(\n",
            "    (input_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "    (post_attention_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "    (shared_mlp): GraniteMoeHybridMLP(\n",
            "      (activation): SiLUActivation()\n",
            "      (input_linear): Linear(in_features=768, out_features=4096, bias=False)\n",
            "      (output_linear): Linear(in_features=2048, out_features=768, bias=False)\n",
            "    )\n",
            "    (mamba): GraniteMoeHybridMambaLayer(\n",
            "      (act): SiLUActivation()\n",
            "      (conv1d): Conv1d(1792, 1792, kernel_size=(4,), stride=(1,), padding=(3,), groups=1792)\n",
            "      (in_proj): Linear(in_features=768, out_features=3376, bias=False)\n",
            "      (norm): GraniteMoeHybridRMSNormGated()\n",
            "      (out_proj): Linear(in_features=1536, out_features=768, bias=False)\n",
            "    )\n",
            "  )\n",
            "  (13): GraniteMoeHybridDecoderLayer(\n",
            "    (input_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "    (post_attention_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "    (shared_mlp): GraniteMoeHybridMLP(\n",
            "      (activation): SiLUActivation()\n",
            "      (input_linear): Linear(in_features=768, out_features=4096, bias=False)\n",
            "      (output_linear): Linear(in_features=2048, out_features=768, bias=False)\n",
            "    )\n",
            "    (self_attn): GraniteMoeHybridAttention(\n",
            "      (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
            "      (k_proj): Linear(in_features=768, out_features=256, bias=False)\n",
            "      (v_proj): Linear(in_features=768, out_features=256, bias=False)\n",
            "      (o_proj): Linear(in_features=768, out_features=768, bias=False)\n",
            "    )\n",
            "  )\n",
            "  (14-16): 3 x GraniteMoeHybridDecoderLayer(\n",
            "    (input_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "    (post_attention_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "    (shared_mlp): GraniteMoeHybridMLP(\n",
            "      (activation): SiLUActivation()\n",
            "      (input_linear): Linear(in_features=768, out_features=4096, bias=False)\n",
            "      (output_linear): Linear(in_features=2048, out_features=768, bias=False)\n",
            "    )\n",
            "    (mamba): GraniteMoeHybridMambaLayer(\n",
            "      (act): SiLUActivation()\n",
            "      (conv1d): Conv1d(1792, 1792, kernel_size=(4,), stride=(1,), padding=(3,), groups=1792)\n",
            "      (in_proj): Linear(in_features=768, out_features=3376, bias=False)\n",
            "      (norm): GraniteMoeHybridRMSNormGated()\n",
            "      (out_proj): Linear(in_features=1536, out_features=768, bias=False)\n",
            "    )\n",
            "  )\n",
            "  (17): GraniteMoeHybridDecoderLayer(\n",
            "    (input_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "    (post_attention_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "    (shared_mlp): GraniteMoeHybridMLP(\n",
            "      (activation): SiLUActivation()\n",
            "      (input_linear): Linear(in_features=768, out_features=4096, bias=False)\n",
            "      (output_linear): Linear(in_features=2048, out_features=768, bias=False)\n",
            "    )\n",
            "    (self_attn): GraniteMoeHybridAttention(\n",
            "      (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
            "      (k_proj): Linear(in_features=768, out_features=256, bias=False)\n",
            "      (v_proj): Linear(in_features=768, out_features=256, bias=False)\n",
            "      (o_proj): Linear(in_features=768, out_features=768, bias=False)\n",
            "    )\n",
            "  )\n",
            "  (18-26): 9 x GraniteMoeHybridDecoderLayer(\n",
            "    (input_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "    (post_attention_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "    (shared_mlp): GraniteMoeHybridMLP(\n",
            "      (activation): SiLUActivation()\n",
            "      (input_linear): Linear(in_features=768, out_features=4096, bias=False)\n",
            "      (output_linear): Linear(in_features=2048, out_features=768, bias=False)\n",
            "    )\n",
            "    (mamba): GraniteMoeHybridMambaLayer(\n",
            "      (act): SiLUActivation()\n",
            "      (conv1d): Conv1d(1792, 1792, kernel_size=(4,), stride=(1,), padding=(3,), groups=1792)\n",
            "      (in_proj): Linear(in_features=768, out_features=3376, bias=False)\n",
            "      (norm): GraniteMoeHybridRMSNormGated()\n",
            "      (out_proj): Linear(in_features=1536, out_features=768, bias=False)\n",
            "    )\n",
            "  )\n",
            "  (27): GraniteMoeHybridDecoderLayer(\n",
            "    (input_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "    (post_attention_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "    (shared_mlp): GraniteMoeHybridMLP(\n",
            "      (activation): SiLUActivation()\n",
            "      (input_linear): Linear(in_features=768, out_features=4096, bias=False)\n",
            "      (output_linear): Linear(in_features=2048, out_features=768, bias=False)\n",
            "    )\n",
            "    (self_attn): GraniteMoeHybridAttention(\n",
            "      (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
            "      (k_proj): Linear(in_features=768, out_features=256, bias=False)\n",
            "      (v_proj): Linear(in_features=768, out_features=256, bias=False)\n",
            "      (o_proj): Linear(in_features=768, out_features=768, bias=False)\n",
            "    )\n",
            "  )\n",
            "  (28-31): 4 x GraniteMoeHybridDecoderLayer(\n",
            "    (input_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "    (post_attention_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "    (shared_mlp): GraniteMoeHybridMLP(\n",
            "      (activation): SiLUActivation()\n",
            "      (input_linear): Linear(in_features=768, out_features=4096, bias=False)\n",
            "      (output_linear): Linear(in_features=2048, out_features=768, bias=False)\n",
            "    )\n",
            "    (mamba): GraniteMoeHybridMambaLayer(\n",
            "      (act): SiLUActivation()\n",
            "      (conv1d): Conv1d(1792, 1792, kernel_size=(4,), stride=(1,), padding=(3,), groups=1792)\n",
            "      (in_proj): Linear(in_features=768, out_features=3376, bias=False)\n",
            "      (norm): GraniteMoeHybridRMSNormGated()\n",
            "      (out_proj): Linear(in_features=1536, out_features=768, bias=False)\n",
            "    )\n",
            "  )\n",
            ")\n",
            "model.layers.0 -> GraniteMoeHybridDecoderLayer(\n",
            "  (input_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "  (post_attention_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "  (shared_mlp): GraniteMoeHybridMLP(\n",
            "    (activation): SiLUActivation()\n",
            "    (input_linear): Linear(in_features=768, out_features=4096, bias=False)\n",
            "    (output_linear): Linear(in_features=2048, out_features=768, bias=False)\n",
            "  )\n",
            "  (mamba): GraniteMoeHybridMambaLayer(\n",
            "    (act): SiLUActivation()\n",
            "    (conv1d): Conv1d(1792, 1792, kernel_size=(4,), stride=(1,), padding=(3,), groups=1792)\n",
            "    (in_proj): Linear(in_features=768, out_features=3376, bias=False)\n",
            "    (norm): GraniteMoeHybridRMSNormGated()\n",
            "    (out_proj): Linear(in_features=1536, out_features=768, bias=False)\n",
            "  )\n",
            ")\n",
            "model.layers.0.input_layernorm -> GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "model.layers.0.post_attention_layernorm -> GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "model.layers.0.shared_mlp -> GraniteMoeHybridMLP(\n",
            "  (activation): SiLUActivation()\n",
            "  (input_linear): Linear(in_features=768, out_features=4096, bias=False)\n",
            "  (output_linear): Linear(in_features=2048, out_features=768, bias=False)\n",
            ")\n",
            "model.layers.0.shared_mlp.activation -> SiLUActivation()\n",
            "model.layers.0.shared_mlp.input_linear -> Linear(in_features=768, out_features=4096, bias=False)\n",
            "model.layers.0.shared_mlp.output_linear -> Linear(in_features=2048, out_features=768, bias=False)\n",
            "model.layers.0.mamba -> GraniteMoeHybridMambaLayer(\n",
            "  (act): SiLUActivation()\n",
            "  (conv1d): Conv1d(1792, 1792, kernel_size=(4,), stride=(1,), padding=(3,), groups=1792)\n",
            "  (in_proj): Linear(in_features=768, out_features=3376, bias=False)\n",
            "  (norm): GraniteMoeHybridRMSNormGated()\n",
            "  (out_proj): Linear(in_features=1536, out_features=768, bias=False)\n",
            ")\n",
            "model.layers.0.mamba.act -> SiLUActivation()\n",
            "model.layers.0.mamba.conv1d -> Conv1d(1792, 1792, kernel_size=(4,), stride=(1,), padding=(3,), groups=1792)\n",
            "model.layers.0.mamba.in_proj -> Linear(in_features=768, out_features=3376, bias=False)\n",
            "model.layers.0.mamba.norm -> GraniteMoeHybridRMSNormGated()\n",
            "model.layers.0.mamba.out_proj -> Linear(in_features=1536, out_features=768, bias=False)\n",
            "model.layers.1 -> GraniteMoeHybridDecoderLayer(\n",
            "  (input_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "  (post_attention_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "  (shared_mlp): GraniteMoeHybridMLP(\n",
            "    (activation): SiLUActivation()\n",
            "    (input_linear): Linear(in_features=768, out_features=4096, bias=False)\n",
            "    (output_linear): Linear(in_features=2048, out_features=768, bias=False)\n",
            "  )\n",
            "  (mamba): GraniteMoeHybridMambaLayer(\n",
            "    (act): SiLUActivation()\n",
            "    (conv1d): Conv1d(1792, 1792, kernel_size=(4,), stride=(1,), padding=(3,), groups=1792)\n",
            "    (in_proj): Linear(in_features=768, out_features=3376, bias=False)\n",
            "    (norm): GraniteMoeHybridRMSNormGated()\n",
            "    (out_proj): Linear(in_features=1536, out_features=768, bias=False)\n",
            "  )\n",
            ")\n",
            "model.layers.1.input_layernorm -> GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "model.layers.1.post_attention_layernorm -> GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "model.layers.1.shared_mlp -> GraniteMoeHybridMLP(\n",
            "  (activation): SiLUActivation()\n",
            "  (input_linear): Linear(in_features=768, out_features=4096, bias=False)\n",
            "  (output_linear): Linear(in_features=2048, out_features=768, bias=False)\n",
            ")\n",
            "model.layers.1.shared_mlp.activation -> SiLUActivation()\n",
            "model.layers.1.shared_mlp.input_linear -> Linear(in_features=768, out_features=4096, bias=False)\n",
            "model.layers.1.shared_mlp.output_linear -> Linear(in_features=2048, out_features=768, bias=False)\n",
            "model.layers.1.mamba -> GraniteMoeHybridMambaLayer(\n",
            "  (act): SiLUActivation()\n",
            "  (conv1d): Conv1d(1792, 1792, kernel_size=(4,), stride=(1,), padding=(3,), groups=1792)\n",
            "  (in_proj): Linear(in_features=768, out_features=3376, bias=False)\n",
            "  (norm): GraniteMoeHybridRMSNormGated()\n",
            "  (out_proj): Linear(in_features=1536, out_features=768, bias=False)\n",
            ")\n",
            "model.layers.1.mamba.act -> SiLUActivation()\n",
            "model.layers.1.mamba.conv1d -> Conv1d(1792, 1792, kernel_size=(4,), stride=(1,), padding=(3,), groups=1792)\n",
            "model.layers.1.mamba.in_proj -> Linear(in_features=768, out_features=3376, bias=False)\n",
            "model.layers.1.mamba.norm -> GraniteMoeHybridRMSNormGated()\n",
            "model.layers.1.mamba.out_proj -> Linear(in_features=1536, out_features=768, bias=False)\n",
            "model.layers.2 -> GraniteMoeHybridDecoderLayer(\n",
            "  (input_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "  (post_attention_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "  (shared_mlp): GraniteMoeHybridMLP(\n",
            "    (activation): SiLUActivation()\n",
            "    (input_linear): Linear(in_features=768, out_features=4096, bias=False)\n",
            "    (output_linear): Linear(in_features=2048, out_features=768, bias=False)\n",
            "  )\n",
            "  (mamba): GraniteMoeHybridMambaLayer(\n",
            "    (act): SiLUActivation()\n",
            "    (conv1d): Conv1d(1792, 1792, kernel_size=(4,), stride=(1,), padding=(3,), groups=1792)\n",
            "    (in_proj): Linear(in_features=768, out_features=3376, bias=False)\n",
            "    (norm): GraniteMoeHybridRMSNormGated()\n",
            "    (out_proj): Linear(in_features=1536, out_features=768, bias=False)\n",
            "  )\n",
            ")\n",
            "model.layers.2.input_layernorm -> GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "model.layers.2.post_attention_layernorm -> GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "model.layers.2.shared_mlp -> GraniteMoeHybridMLP(\n",
            "  (activation): SiLUActivation()\n",
            "  (input_linear): Linear(in_features=768, out_features=4096, bias=False)\n",
            "  (output_linear): Linear(in_features=2048, out_features=768, bias=False)\n",
            ")\n",
            "model.layers.2.shared_mlp.activation -> SiLUActivation()\n",
            "model.layers.2.shared_mlp.input_linear -> Linear(in_features=768, out_features=4096, bias=False)\n",
            "model.layers.2.shared_mlp.output_linear -> Linear(in_features=2048, out_features=768, bias=False)\n",
            "model.layers.2.mamba -> GraniteMoeHybridMambaLayer(\n",
            "  (act): SiLUActivation()\n",
            "  (conv1d): Conv1d(1792, 1792, kernel_size=(4,), stride=(1,), padding=(3,), groups=1792)\n",
            "  (in_proj): Linear(in_features=768, out_features=3376, bias=False)\n",
            "  (norm): GraniteMoeHybridRMSNormGated()\n",
            "  (out_proj): Linear(in_features=1536, out_features=768, bias=False)\n",
            ")\n",
            "model.layers.2.mamba.act -> SiLUActivation()\n",
            "model.layers.2.mamba.conv1d -> Conv1d(1792, 1792, kernel_size=(4,), stride=(1,), padding=(3,), groups=1792)\n",
            "model.layers.2.mamba.in_proj -> Linear(in_features=768, out_features=3376, bias=False)\n",
            "model.layers.2.mamba.norm -> GraniteMoeHybridRMSNormGated()\n",
            "model.layers.2.mamba.out_proj -> Linear(in_features=1536, out_features=768, bias=False)\n",
            "model.layers.3 -> GraniteMoeHybridDecoderLayer(\n",
            "  (input_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "  (post_attention_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "  (shared_mlp): GraniteMoeHybridMLP(\n",
            "    (activation): SiLUActivation()\n",
            "    (input_linear): Linear(in_features=768, out_features=4096, bias=False)\n",
            "    (output_linear): Linear(in_features=2048, out_features=768, bias=False)\n",
            "  )\n",
            "  (mamba): GraniteMoeHybridMambaLayer(\n",
            "    (act): SiLUActivation()\n",
            "    (conv1d): Conv1d(1792, 1792, kernel_size=(4,), stride=(1,), padding=(3,), groups=1792)\n",
            "    (in_proj): Linear(in_features=768, out_features=3376, bias=False)\n",
            "    (norm): GraniteMoeHybridRMSNormGated()\n",
            "    (out_proj): Linear(in_features=1536, out_features=768, bias=False)\n",
            "  )\n",
            ")\n",
            "model.layers.3.input_layernorm -> GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "model.layers.3.post_attention_layernorm -> GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "model.layers.3.shared_mlp -> GraniteMoeHybridMLP(\n",
            "  (activation): SiLUActivation()\n",
            "  (input_linear): Linear(in_features=768, out_features=4096, bias=False)\n",
            "  (output_linear): Linear(in_features=2048, out_features=768, bias=False)\n",
            ")\n",
            "model.layers.3.shared_mlp.activation -> SiLUActivation()\n",
            "model.layers.3.shared_mlp.input_linear -> Linear(in_features=768, out_features=4096, bias=False)\n",
            "model.layers.3.shared_mlp.output_linear -> Linear(in_features=2048, out_features=768, bias=False)\n",
            "model.layers.3.mamba -> GraniteMoeHybridMambaLayer(\n",
            "  (act): SiLUActivation()\n",
            "  (conv1d): Conv1d(1792, 1792, kernel_size=(4,), stride=(1,), padding=(3,), groups=1792)\n",
            "  (in_proj): Linear(in_features=768, out_features=3376, bias=False)\n",
            "  (norm): GraniteMoeHybridRMSNormGated()\n",
            "  (out_proj): Linear(in_features=1536, out_features=768, bias=False)\n",
            ")\n",
            "model.layers.3.mamba.act -> SiLUActivation()\n",
            "model.layers.3.mamba.conv1d -> Conv1d(1792, 1792, kernel_size=(4,), stride=(1,), padding=(3,), groups=1792)\n",
            "model.layers.3.mamba.in_proj -> Linear(in_features=768, out_features=3376, bias=False)\n",
            "model.layers.3.mamba.norm -> GraniteMoeHybridRMSNormGated()\n",
            "model.layers.3.mamba.out_proj -> Linear(in_features=1536, out_features=768, bias=False)\n",
            "model.layers.4 -> GraniteMoeHybridDecoderLayer(\n",
            "  (input_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "  (post_attention_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "  (shared_mlp): GraniteMoeHybridMLP(\n",
            "    (activation): SiLUActivation()\n",
            "    (input_linear): Linear(in_features=768, out_features=4096, bias=False)\n",
            "    (output_linear): Linear(in_features=2048, out_features=768, bias=False)\n",
            "  )\n",
            "  (mamba): GraniteMoeHybridMambaLayer(\n",
            "    (act): SiLUActivation()\n",
            "    (conv1d): Conv1d(1792, 1792, kernel_size=(4,), stride=(1,), padding=(3,), groups=1792)\n",
            "    (in_proj): Linear(in_features=768, out_features=3376, bias=False)\n",
            "    (norm): GraniteMoeHybridRMSNormGated()\n",
            "    (out_proj): Linear(in_features=1536, out_features=768, bias=False)\n",
            "  )\n",
            ")\n",
            "model.layers.4.input_layernorm -> GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "model.layers.4.post_attention_layernorm -> GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "model.layers.4.shared_mlp -> GraniteMoeHybridMLP(\n",
            "  (activation): SiLUActivation()\n",
            "  (input_linear): Linear(in_features=768, out_features=4096, bias=False)\n",
            "  (output_linear): Linear(in_features=2048, out_features=768, bias=False)\n",
            ")\n",
            "model.layers.4.shared_mlp.activation -> SiLUActivation()\n",
            "model.layers.4.shared_mlp.input_linear -> Linear(in_features=768, out_features=4096, bias=False)\n",
            "model.layers.4.shared_mlp.output_linear -> Linear(in_features=2048, out_features=768, bias=False)\n",
            "model.layers.4.mamba -> GraniteMoeHybridMambaLayer(\n",
            "  (act): SiLUActivation()\n",
            "  (conv1d): Conv1d(1792, 1792, kernel_size=(4,), stride=(1,), padding=(3,), groups=1792)\n",
            "  (in_proj): Linear(in_features=768, out_features=3376, bias=False)\n",
            "  (norm): GraniteMoeHybridRMSNormGated()\n",
            "  (out_proj): Linear(in_features=1536, out_features=768, bias=False)\n",
            ")\n",
            "model.layers.4.mamba.act -> SiLUActivation()\n",
            "model.layers.4.mamba.conv1d -> Conv1d(1792, 1792, kernel_size=(4,), stride=(1,), padding=(3,), groups=1792)\n",
            "model.layers.4.mamba.in_proj -> Linear(in_features=768, out_features=3376, bias=False)\n",
            "model.layers.4.mamba.norm -> GraniteMoeHybridRMSNormGated()\n",
            "model.layers.4.mamba.out_proj -> Linear(in_features=1536, out_features=768, bias=False)\n",
            "model.layers.5 -> GraniteMoeHybridDecoderLayer(\n",
            "  (input_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "  (post_attention_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "  (shared_mlp): GraniteMoeHybridMLP(\n",
            "    (activation): SiLUActivation()\n",
            "    (input_linear): Linear(in_features=768, out_features=4096, bias=False)\n",
            "    (output_linear): Linear(in_features=2048, out_features=768, bias=False)\n",
            "  )\n",
            "  (mamba): GraniteMoeHybridMambaLayer(\n",
            "    (act): SiLUActivation()\n",
            "    (conv1d): Conv1d(1792, 1792, kernel_size=(4,), stride=(1,), padding=(3,), groups=1792)\n",
            "    (in_proj): Linear(in_features=768, out_features=3376, bias=False)\n",
            "    (norm): GraniteMoeHybridRMSNormGated()\n",
            "    (out_proj): Linear(in_features=1536, out_features=768, bias=False)\n",
            "  )\n",
            ")\n",
            "model.layers.5.input_layernorm -> GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "model.layers.5.post_attention_layernorm -> GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "model.layers.5.shared_mlp -> GraniteMoeHybridMLP(\n",
            "  (activation): SiLUActivation()\n",
            "  (input_linear): Linear(in_features=768, out_features=4096, bias=False)\n",
            "  (output_linear): Linear(in_features=2048, out_features=768, bias=False)\n",
            ")\n",
            "model.layers.5.shared_mlp.activation -> SiLUActivation()\n",
            "model.layers.5.shared_mlp.input_linear -> Linear(in_features=768, out_features=4096, bias=False)\n",
            "model.layers.5.shared_mlp.output_linear -> Linear(in_features=2048, out_features=768, bias=False)\n",
            "model.layers.5.mamba -> GraniteMoeHybridMambaLayer(\n",
            "  (act): SiLUActivation()\n",
            "  (conv1d): Conv1d(1792, 1792, kernel_size=(4,), stride=(1,), padding=(3,), groups=1792)\n",
            "  (in_proj): Linear(in_features=768, out_features=3376, bias=False)\n",
            "  (norm): GraniteMoeHybridRMSNormGated()\n",
            "  (out_proj): Linear(in_features=1536, out_features=768, bias=False)\n",
            ")\n",
            "model.layers.5.mamba.act -> SiLUActivation()\n",
            "model.layers.5.mamba.conv1d -> Conv1d(1792, 1792, kernel_size=(4,), stride=(1,), padding=(3,), groups=1792)\n",
            "model.layers.5.mamba.in_proj -> Linear(in_features=768, out_features=3376, bias=False)\n",
            "model.layers.5.mamba.norm -> GraniteMoeHybridRMSNormGated()\n",
            "model.layers.5.mamba.out_proj -> Linear(in_features=1536, out_features=768, bias=False)\n",
            "model.layers.6 -> GraniteMoeHybridDecoderLayer(\n",
            "  (input_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "  (post_attention_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "  (shared_mlp): GraniteMoeHybridMLP(\n",
            "    (activation): SiLUActivation()\n",
            "    (input_linear): Linear(in_features=768, out_features=4096, bias=False)\n",
            "    (output_linear): Linear(in_features=2048, out_features=768, bias=False)\n",
            "  )\n",
            "  (mamba): GraniteMoeHybridMambaLayer(\n",
            "    (act): SiLUActivation()\n",
            "    (conv1d): Conv1d(1792, 1792, kernel_size=(4,), stride=(1,), padding=(3,), groups=1792)\n",
            "    (in_proj): Linear(in_features=768, out_features=3376, bias=False)\n",
            "    (norm): GraniteMoeHybridRMSNormGated()\n",
            "    (out_proj): Linear(in_features=1536, out_features=768, bias=False)\n",
            "  )\n",
            ")\n",
            "model.layers.6.input_layernorm -> GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "model.layers.6.post_attention_layernorm -> GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "model.layers.6.shared_mlp -> GraniteMoeHybridMLP(\n",
            "  (activation): SiLUActivation()\n",
            "  (input_linear): Linear(in_features=768, out_features=4096, bias=False)\n",
            "  (output_linear): Linear(in_features=2048, out_features=768, bias=False)\n",
            ")\n",
            "model.layers.6.shared_mlp.activation -> SiLUActivation()\n",
            "model.layers.6.shared_mlp.input_linear -> Linear(in_features=768, out_features=4096, bias=False)\n",
            "model.layers.6.shared_mlp.output_linear -> Linear(in_features=2048, out_features=768, bias=False)\n",
            "model.layers.6.mamba -> GraniteMoeHybridMambaLayer(\n",
            "  (act): SiLUActivation()\n",
            "  (conv1d): Conv1d(1792, 1792, kernel_size=(4,), stride=(1,), padding=(3,), groups=1792)\n",
            "  (in_proj): Linear(in_features=768, out_features=3376, bias=False)\n",
            "  (norm): GraniteMoeHybridRMSNormGated()\n",
            "  (out_proj): Linear(in_features=1536, out_features=768, bias=False)\n",
            ")\n",
            "model.layers.6.mamba.act -> SiLUActivation()\n",
            "model.layers.6.mamba.conv1d -> Conv1d(1792, 1792, kernel_size=(4,), stride=(1,), padding=(3,), groups=1792)\n",
            "model.layers.6.mamba.in_proj -> Linear(in_features=768, out_features=3376, bias=False)\n",
            "model.layers.6.mamba.norm -> GraniteMoeHybridRMSNormGated()\n",
            "model.layers.6.mamba.out_proj -> Linear(in_features=1536, out_features=768, bias=False)\n",
            "model.layers.7 -> GraniteMoeHybridDecoderLayer(\n",
            "  (input_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "  (post_attention_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "  (shared_mlp): GraniteMoeHybridMLP(\n",
            "    (activation): SiLUActivation()\n",
            "    (input_linear): Linear(in_features=768, out_features=4096, bias=False)\n",
            "    (output_linear): Linear(in_features=2048, out_features=768, bias=False)\n",
            "  )\n",
            "  (mamba): GraniteMoeHybridMambaLayer(\n",
            "    (act): SiLUActivation()\n",
            "    (conv1d): Conv1d(1792, 1792, kernel_size=(4,), stride=(1,), padding=(3,), groups=1792)\n",
            "    (in_proj): Linear(in_features=768, out_features=3376, bias=False)\n",
            "    (norm): GraniteMoeHybridRMSNormGated()\n",
            "    (out_proj): Linear(in_features=1536, out_features=768, bias=False)\n",
            "  )\n",
            ")\n",
            "model.layers.7.input_layernorm -> GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "model.layers.7.post_attention_layernorm -> GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "model.layers.7.shared_mlp -> GraniteMoeHybridMLP(\n",
            "  (activation): SiLUActivation()\n",
            "  (input_linear): Linear(in_features=768, out_features=4096, bias=False)\n",
            "  (output_linear): Linear(in_features=2048, out_features=768, bias=False)\n",
            ")\n",
            "model.layers.7.shared_mlp.activation -> SiLUActivation()\n",
            "model.layers.7.shared_mlp.input_linear -> Linear(in_features=768, out_features=4096, bias=False)\n",
            "model.layers.7.shared_mlp.output_linear -> Linear(in_features=2048, out_features=768, bias=False)\n",
            "model.layers.7.mamba -> GraniteMoeHybridMambaLayer(\n",
            "  (act): SiLUActivation()\n",
            "  (conv1d): Conv1d(1792, 1792, kernel_size=(4,), stride=(1,), padding=(3,), groups=1792)\n",
            "  (in_proj): Linear(in_features=768, out_features=3376, bias=False)\n",
            "  (norm): GraniteMoeHybridRMSNormGated()\n",
            "  (out_proj): Linear(in_features=1536, out_features=768, bias=False)\n",
            ")\n",
            "model.layers.7.mamba.act -> SiLUActivation()\n",
            "model.layers.7.mamba.conv1d -> Conv1d(1792, 1792, kernel_size=(4,), stride=(1,), padding=(3,), groups=1792)\n",
            "model.layers.7.mamba.in_proj -> Linear(in_features=768, out_features=3376, bias=False)\n",
            "model.layers.7.mamba.norm -> GraniteMoeHybridRMSNormGated()\n",
            "model.layers.7.mamba.out_proj -> Linear(in_features=1536, out_features=768, bias=False)\n",
            "model.layers.8 -> GraniteMoeHybridDecoderLayer(\n",
            "  (input_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "  (post_attention_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "  (shared_mlp): GraniteMoeHybridMLP(\n",
            "    (activation): SiLUActivation()\n",
            "    (input_linear): Linear(in_features=768, out_features=4096, bias=False)\n",
            "    (output_linear): Linear(in_features=2048, out_features=768, bias=False)\n",
            "  )\n",
            "  (mamba): GraniteMoeHybridMambaLayer(\n",
            "    (act): SiLUActivation()\n",
            "    (conv1d): Conv1d(1792, 1792, kernel_size=(4,), stride=(1,), padding=(3,), groups=1792)\n",
            "    (in_proj): Linear(in_features=768, out_features=3376, bias=False)\n",
            "    (norm): GraniteMoeHybridRMSNormGated()\n",
            "    (out_proj): Linear(in_features=1536, out_features=768, bias=False)\n",
            "  )\n",
            ")\n",
            "model.layers.8.input_layernorm -> GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "model.layers.8.post_attention_layernorm -> GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "model.layers.8.shared_mlp -> GraniteMoeHybridMLP(\n",
            "  (activation): SiLUActivation()\n",
            "  (input_linear): Linear(in_features=768, out_features=4096, bias=False)\n",
            "  (output_linear): Linear(in_features=2048, out_features=768, bias=False)\n",
            ")\n",
            "model.layers.8.shared_mlp.activation -> SiLUActivation()\n",
            "model.layers.8.shared_mlp.input_linear -> Linear(in_features=768, out_features=4096, bias=False)\n",
            "model.layers.8.shared_mlp.output_linear -> Linear(in_features=2048, out_features=768, bias=False)\n",
            "model.layers.8.mamba -> GraniteMoeHybridMambaLayer(\n",
            "  (act): SiLUActivation()\n",
            "  (conv1d): Conv1d(1792, 1792, kernel_size=(4,), stride=(1,), padding=(3,), groups=1792)\n",
            "  (in_proj): Linear(in_features=768, out_features=3376, bias=False)\n",
            "  (norm): GraniteMoeHybridRMSNormGated()\n",
            "  (out_proj): Linear(in_features=1536, out_features=768, bias=False)\n",
            ")\n",
            "model.layers.8.mamba.act -> SiLUActivation()\n",
            "model.layers.8.mamba.conv1d -> Conv1d(1792, 1792, kernel_size=(4,), stride=(1,), padding=(3,), groups=1792)\n",
            "model.layers.8.mamba.in_proj -> Linear(in_features=768, out_features=3376, bias=False)\n",
            "model.layers.8.mamba.norm -> GraniteMoeHybridRMSNormGated()\n",
            "model.layers.8.mamba.out_proj -> Linear(in_features=1536, out_features=768, bias=False)\n",
            "model.layers.9 -> GraniteMoeHybridDecoderLayer(\n",
            "  (input_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "  (post_attention_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "  (shared_mlp): GraniteMoeHybridMLP(\n",
            "    (activation): SiLUActivation()\n",
            "    (input_linear): Linear(in_features=768, out_features=4096, bias=False)\n",
            "    (output_linear): Linear(in_features=2048, out_features=768, bias=False)\n",
            "  )\n",
            "  (mamba): GraniteMoeHybridMambaLayer(\n",
            "    (act): SiLUActivation()\n",
            "    (conv1d): Conv1d(1792, 1792, kernel_size=(4,), stride=(1,), padding=(3,), groups=1792)\n",
            "    (in_proj): Linear(in_features=768, out_features=3376, bias=False)\n",
            "    (norm): GraniteMoeHybridRMSNormGated()\n",
            "    (out_proj): Linear(in_features=1536, out_features=768, bias=False)\n",
            "  )\n",
            ")\n",
            "model.layers.9.input_layernorm -> GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "model.layers.9.post_attention_layernorm -> GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "model.layers.9.shared_mlp -> GraniteMoeHybridMLP(\n",
            "  (activation): SiLUActivation()\n",
            "  (input_linear): Linear(in_features=768, out_features=4096, bias=False)\n",
            "  (output_linear): Linear(in_features=2048, out_features=768, bias=False)\n",
            ")\n",
            "model.layers.9.shared_mlp.activation -> SiLUActivation()\n",
            "model.layers.9.shared_mlp.input_linear -> Linear(in_features=768, out_features=4096, bias=False)\n",
            "model.layers.9.shared_mlp.output_linear -> Linear(in_features=2048, out_features=768, bias=False)\n",
            "model.layers.9.mamba -> GraniteMoeHybridMambaLayer(\n",
            "  (act): SiLUActivation()\n",
            "  (conv1d): Conv1d(1792, 1792, kernel_size=(4,), stride=(1,), padding=(3,), groups=1792)\n",
            "  (in_proj): Linear(in_features=768, out_features=3376, bias=False)\n",
            "  (norm): GraniteMoeHybridRMSNormGated()\n",
            "  (out_proj): Linear(in_features=1536, out_features=768, bias=False)\n",
            ")\n",
            "model.layers.9.mamba.act -> SiLUActivation()\n",
            "model.layers.9.mamba.conv1d -> Conv1d(1792, 1792, kernel_size=(4,), stride=(1,), padding=(3,), groups=1792)\n",
            "model.layers.9.mamba.in_proj -> Linear(in_features=768, out_features=3376, bias=False)\n",
            "model.layers.9.mamba.norm -> GraniteMoeHybridRMSNormGated()\n",
            "model.layers.9.mamba.out_proj -> Linear(in_features=1536, out_features=768, bias=False)\n",
            "model.layers.10 -> GraniteMoeHybridDecoderLayer(\n",
            "  (input_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "  (post_attention_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "  (shared_mlp): GraniteMoeHybridMLP(\n",
            "    (activation): SiLUActivation()\n",
            "    (input_linear): Linear(in_features=768, out_features=4096, bias=False)\n",
            "    (output_linear): Linear(in_features=2048, out_features=768, bias=False)\n",
            "  )\n",
            "  (self_attn): GraniteMoeHybridAttention(\n",
            "    (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
            "    (k_proj): Linear(in_features=768, out_features=256, bias=False)\n",
            "    (v_proj): Linear(in_features=768, out_features=256, bias=False)\n",
            "    (o_proj): Linear(in_features=768, out_features=768, bias=False)\n",
            "  )\n",
            ")\n",
            "model.layers.10.input_layernorm -> GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "model.layers.10.post_attention_layernorm -> GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "model.layers.10.shared_mlp -> GraniteMoeHybridMLP(\n",
            "  (activation): SiLUActivation()\n",
            "  (input_linear): Linear(in_features=768, out_features=4096, bias=False)\n",
            "  (output_linear): Linear(in_features=2048, out_features=768, bias=False)\n",
            ")\n",
            "model.layers.10.shared_mlp.activation -> SiLUActivation()\n",
            "model.layers.10.shared_mlp.input_linear -> Linear(in_features=768, out_features=4096, bias=False)\n",
            "model.layers.10.shared_mlp.output_linear -> Linear(in_features=2048, out_features=768, bias=False)\n",
            "model.layers.10.self_attn -> GraniteMoeHybridAttention(\n",
            "  (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
            "  (k_proj): Linear(in_features=768, out_features=256, bias=False)\n",
            "  (v_proj): Linear(in_features=768, out_features=256, bias=False)\n",
            "  (o_proj): Linear(in_features=768, out_features=768, bias=False)\n",
            ")\n",
            "model.layers.10.self_attn.q_proj -> Linear(in_features=768, out_features=768, bias=False)\n",
            "model.layers.10.self_attn.k_proj -> Linear(in_features=768, out_features=256, bias=False)\n",
            "model.layers.10.self_attn.v_proj -> Linear(in_features=768, out_features=256, bias=False)\n",
            "model.layers.10.self_attn.o_proj -> Linear(in_features=768, out_features=768, bias=False)\n",
            "model.layers.11 -> GraniteMoeHybridDecoderLayer(\n",
            "  (input_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "  (post_attention_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "  (shared_mlp): GraniteMoeHybridMLP(\n",
            "    (activation): SiLUActivation()\n",
            "    (input_linear): Linear(in_features=768, out_features=4096, bias=False)\n",
            "    (output_linear): Linear(in_features=2048, out_features=768, bias=False)\n",
            "  )\n",
            "  (mamba): GraniteMoeHybridMambaLayer(\n",
            "    (act): SiLUActivation()\n",
            "    (conv1d): Conv1d(1792, 1792, kernel_size=(4,), stride=(1,), padding=(3,), groups=1792)\n",
            "    (in_proj): Linear(in_features=768, out_features=3376, bias=False)\n",
            "    (norm): GraniteMoeHybridRMSNormGated()\n",
            "    (out_proj): Linear(in_features=1536, out_features=768, bias=False)\n",
            "  )\n",
            ")\n",
            "model.layers.11.input_layernorm -> GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "model.layers.11.post_attention_layernorm -> GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "model.layers.11.shared_mlp -> GraniteMoeHybridMLP(\n",
            "  (activation): SiLUActivation()\n",
            "  (input_linear): Linear(in_features=768, out_features=4096, bias=False)\n",
            "  (output_linear): Linear(in_features=2048, out_features=768, bias=False)\n",
            ")\n",
            "model.layers.11.shared_mlp.activation -> SiLUActivation()\n",
            "model.layers.11.shared_mlp.input_linear -> Linear(in_features=768, out_features=4096, bias=False)\n",
            "model.layers.11.shared_mlp.output_linear -> Linear(in_features=2048, out_features=768, bias=False)\n",
            "model.layers.11.mamba -> GraniteMoeHybridMambaLayer(\n",
            "  (act): SiLUActivation()\n",
            "  (conv1d): Conv1d(1792, 1792, kernel_size=(4,), stride=(1,), padding=(3,), groups=1792)\n",
            "  (in_proj): Linear(in_features=768, out_features=3376, bias=False)\n",
            "  (norm): GraniteMoeHybridRMSNormGated()\n",
            "  (out_proj): Linear(in_features=1536, out_features=768, bias=False)\n",
            ")\n",
            "model.layers.11.mamba.act -> SiLUActivation()\n",
            "model.layers.11.mamba.conv1d -> Conv1d(1792, 1792, kernel_size=(4,), stride=(1,), padding=(3,), groups=1792)\n",
            "model.layers.11.mamba.in_proj -> Linear(in_features=768, out_features=3376, bias=False)\n",
            "model.layers.11.mamba.norm -> GraniteMoeHybridRMSNormGated()\n",
            "model.layers.11.mamba.out_proj -> Linear(in_features=1536, out_features=768, bias=False)\n",
            "model.layers.12 -> GraniteMoeHybridDecoderLayer(\n",
            "  (input_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "  (post_attention_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "  (shared_mlp): GraniteMoeHybridMLP(\n",
            "    (activation): SiLUActivation()\n",
            "    (input_linear): Linear(in_features=768, out_features=4096, bias=False)\n",
            "    (output_linear): Linear(in_features=2048, out_features=768, bias=False)\n",
            "  )\n",
            "  (mamba): GraniteMoeHybridMambaLayer(\n",
            "    (act): SiLUActivation()\n",
            "    (conv1d): Conv1d(1792, 1792, kernel_size=(4,), stride=(1,), padding=(3,), groups=1792)\n",
            "    (in_proj): Linear(in_features=768, out_features=3376, bias=False)\n",
            "    (norm): GraniteMoeHybridRMSNormGated()\n",
            "    (out_proj): Linear(in_features=1536, out_features=768, bias=False)\n",
            "  )\n",
            ")\n",
            "model.layers.12.input_layernorm -> GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "model.layers.12.post_attention_layernorm -> GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "model.layers.12.shared_mlp -> GraniteMoeHybridMLP(\n",
            "  (activation): SiLUActivation()\n",
            "  (input_linear): Linear(in_features=768, out_features=4096, bias=False)\n",
            "  (output_linear): Linear(in_features=2048, out_features=768, bias=False)\n",
            ")\n",
            "model.layers.12.shared_mlp.activation -> SiLUActivation()\n",
            "model.layers.12.shared_mlp.input_linear -> Linear(in_features=768, out_features=4096, bias=False)\n",
            "model.layers.12.shared_mlp.output_linear -> Linear(in_features=2048, out_features=768, bias=False)\n",
            "model.layers.12.mamba -> GraniteMoeHybridMambaLayer(\n",
            "  (act): SiLUActivation()\n",
            "  (conv1d): Conv1d(1792, 1792, kernel_size=(4,), stride=(1,), padding=(3,), groups=1792)\n",
            "  (in_proj): Linear(in_features=768, out_features=3376, bias=False)\n",
            "  (norm): GraniteMoeHybridRMSNormGated()\n",
            "  (out_proj): Linear(in_features=1536, out_features=768, bias=False)\n",
            ")\n",
            "model.layers.12.mamba.act -> SiLUActivation()\n",
            "model.layers.12.mamba.conv1d -> Conv1d(1792, 1792, kernel_size=(4,), stride=(1,), padding=(3,), groups=1792)\n",
            "model.layers.12.mamba.in_proj -> Linear(in_features=768, out_features=3376, bias=False)\n",
            "model.layers.12.mamba.norm -> GraniteMoeHybridRMSNormGated()\n",
            "model.layers.12.mamba.out_proj -> Linear(in_features=1536, out_features=768, bias=False)\n",
            "model.layers.13 -> GraniteMoeHybridDecoderLayer(\n",
            "  (input_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "  (post_attention_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "  (shared_mlp): GraniteMoeHybridMLP(\n",
            "    (activation): SiLUActivation()\n",
            "    (input_linear): Linear(in_features=768, out_features=4096, bias=False)\n",
            "    (output_linear): Linear(in_features=2048, out_features=768, bias=False)\n",
            "  )\n",
            "  (self_attn): GraniteMoeHybridAttention(\n",
            "    (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
            "    (k_proj): Linear(in_features=768, out_features=256, bias=False)\n",
            "    (v_proj): Linear(in_features=768, out_features=256, bias=False)\n",
            "    (o_proj): Linear(in_features=768, out_features=768, bias=False)\n",
            "  )\n",
            ")\n",
            "model.layers.13.input_layernorm -> GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "model.layers.13.post_attention_layernorm -> GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "model.layers.13.shared_mlp -> GraniteMoeHybridMLP(\n",
            "  (activation): SiLUActivation()\n",
            "  (input_linear): Linear(in_features=768, out_features=4096, bias=False)\n",
            "  (output_linear): Linear(in_features=2048, out_features=768, bias=False)\n",
            ")\n",
            "model.layers.13.shared_mlp.activation -> SiLUActivation()\n",
            "model.layers.13.shared_mlp.input_linear -> Linear(in_features=768, out_features=4096, bias=False)\n",
            "model.layers.13.shared_mlp.output_linear -> Linear(in_features=2048, out_features=768, bias=False)\n",
            "model.layers.13.self_attn -> GraniteMoeHybridAttention(\n",
            "  (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
            "  (k_proj): Linear(in_features=768, out_features=256, bias=False)\n",
            "  (v_proj): Linear(in_features=768, out_features=256, bias=False)\n",
            "  (o_proj): Linear(in_features=768, out_features=768, bias=False)\n",
            ")\n",
            "model.layers.13.self_attn.q_proj -> Linear(in_features=768, out_features=768, bias=False)\n",
            "model.layers.13.self_attn.k_proj -> Linear(in_features=768, out_features=256, bias=False)\n",
            "model.layers.13.self_attn.v_proj -> Linear(in_features=768, out_features=256, bias=False)\n",
            "model.layers.13.self_attn.o_proj -> Linear(in_features=768, out_features=768, bias=False)\n",
            "model.layers.14 -> GraniteMoeHybridDecoderLayer(\n",
            "  (input_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "  (post_attention_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "  (shared_mlp): GraniteMoeHybridMLP(\n",
            "    (activation): SiLUActivation()\n",
            "    (input_linear): Linear(in_features=768, out_features=4096, bias=False)\n",
            "    (output_linear): Linear(in_features=2048, out_features=768, bias=False)\n",
            "  )\n",
            "  (mamba): GraniteMoeHybridMambaLayer(\n",
            "    (act): SiLUActivation()\n",
            "    (conv1d): Conv1d(1792, 1792, kernel_size=(4,), stride=(1,), padding=(3,), groups=1792)\n",
            "    (in_proj): Linear(in_features=768, out_features=3376, bias=False)\n",
            "    (norm): GraniteMoeHybridRMSNormGated()\n",
            "    (out_proj): Linear(in_features=1536, out_features=768, bias=False)\n",
            "  )\n",
            ")\n",
            "model.layers.14.input_layernorm -> GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "model.layers.14.post_attention_layernorm -> GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "model.layers.14.shared_mlp -> GraniteMoeHybridMLP(\n",
            "  (activation): SiLUActivation()\n",
            "  (input_linear): Linear(in_features=768, out_features=4096, bias=False)\n",
            "  (output_linear): Linear(in_features=2048, out_features=768, bias=False)\n",
            ")\n",
            "model.layers.14.shared_mlp.activation -> SiLUActivation()\n",
            "model.layers.14.shared_mlp.input_linear -> Linear(in_features=768, out_features=4096, bias=False)\n",
            "model.layers.14.shared_mlp.output_linear -> Linear(in_features=2048, out_features=768, bias=False)\n",
            "model.layers.14.mamba -> GraniteMoeHybridMambaLayer(\n",
            "  (act): SiLUActivation()\n",
            "  (conv1d): Conv1d(1792, 1792, kernel_size=(4,), stride=(1,), padding=(3,), groups=1792)\n",
            "  (in_proj): Linear(in_features=768, out_features=3376, bias=False)\n",
            "  (norm): GraniteMoeHybridRMSNormGated()\n",
            "  (out_proj): Linear(in_features=1536, out_features=768, bias=False)\n",
            ")\n",
            "model.layers.14.mamba.act -> SiLUActivation()\n",
            "model.layers.14.mamba.conv1d -> Conv1d(1792, 1792, kernel_size=(4,), stride=(1,), padding=(3,), groups=1792)\n",
            "model.layers.14.mamba.in_proj -> Linear(in_features=768, out_features=3376, bias=False)\n",
            "model.layers.14.mamba.norm -> GraniteMoeHybridRMSNormGated()\n",
            "model.layers.14.mamba.out_proj -> Linear(in_features=1536, out_features=768, bias=False)\n",
            "model.layers.15 -> GraniteMoeHybridDecoderLayer(\n",
            "  (input_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "  (post_attention_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "  (shared_mlp): GraniteMoeHybridMLP(\n",
            "    (activation): SiLUActivation()\n",
            "    (input_linear): Linear(in_features=768, out_features=4096, bias=False)\n",
            "    (output_linear): Linear(in_features=2048, out_features=768, bias=False)\n",
            "  )\n",
            "  (mamba): GraniteMoeHybridMambaLayer(\n",
            "    (act): SiLUActivation()\n",
            "    (conv1d): Conv1d(1792, 1792, kernel_size=(4,), stride=(1,), padding=(3,), groups=1792)\n",
            "    (in_proj): Linear(in_features=768, out_features=3376, bias=False)\n",
            "    (norm): GraniteMoeHybridRMSNormGated()\n",
            "    (out_proj): Linear(in_features=1536, out_features=768, bias=False)\n",
            "  )\n",
            ")\n",
            "model.layers.15.input_layernorm -> GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "model.layers.15.post_attention_layernorm -> GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "model.layers.15.shared_mlp -> GraniteMoeHybridMLP(\n",
            "  (activation): SiLUActivation()\n",
            "  (input_linear): Linear(in_features=768, out_features=4096, bias=False)\n",
            "  (output_linear): Linear(in_features=2048, out_features=768, bias=False)\n",
            ")\n",
            "model.layers.15.shared_mlp.activation -> SiLUActivation()\n",
            "model.layers.15.shared_mlp.input_linear -> Linear(in_features=768, out_features=4096, bias=False)\n",
            "model.layers.15.shared_mlp.output_linear -> Linear(in_features=2048, out_features=768, bias=False)\n",
            "model.layers.15.mamba -> GraniteMoeHybridMambaLayer(\n",
            "  (act): SiLUActivation()\n",
            "  (conv1d): Conv1d(1792, 1792, kernel_size=(4,), stride=(1,), padding=(3,), groups=1792)\n",
            "  (in_proj): Linear(in_features=768, out_features=3376, bias=False)\n",
            "  (norm): GraniteMoeHybridRMSNormGated()\n",
            "  (out_proj): Linear(in_features=1536, out_features=768, bias=False)\n",
            ")\n",
            "model.layers.15.mamba.act -> SiLUActivation()\n",
            "model.layers.15.mamba.conv1d -> Conv1d(1792, 1792, kernel_size=(4,), stride=(1,), padding=(3,), groups=1792)\n",
            "model.layers.15.mamba.in_proj -> Linear(in_features=768, out_features=3376, bias=False)\n",
            "model.layers.15.mamba.norm -> GraniteMoeHybridRMSNormGated()\n",
            "model.layers.15.mamba.out_proj -> Linear(in_features=1536, out_features=768, bias=False)\n",
            "model.layers.16 -> GraniteMoeHybridDecoderLayer(\n",
            "  (input_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "  (post_attention_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "  (shared_mlp): GraniteMoeHybridMLP(\n",
            "    (activation): SiLUActivation()\n",
            "    (input_linear): Linear(in_features=768, out_features=4096, bias=False)\n",
            "    (output_linear): Linear(in_features=2048, out_features=768, bias=False)\n",
            "  )\n",
            "  (mamba): GraniteMoeHybridMambaLayer(\n",
            "    (act): SiLUActivation()\n",
            "    (conv1d): Conv1d(1792, 1792, kernel_size=(4,), stride=(1,), padding=(3,), groups=1792)\n",
            "    (in_proj): Linear(in_features=768, out_features=3376, bias=False)\n",
            "    (norm): GraniteMoeHybridRMSNormGated()\n",
            "    (out_proj): Linear(in_features=1536, out_features=768, bias=False)\n",
            "  )\n",
            ")\n",
            "model.layers.16.input_layernorm -> GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "model.layers.16.post_attention_layernorm -> GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "model.layers.16.shared_mlp -> GraniteMoeHybridMLP(\n",
            "  (activation): SiLUActivation()\n",
            "  (input_linear): Linear(in_features=768, out_features=4096, bias=False)\n",
            "  (output_linear): Linear(in_features=2048, out_features=768, bias=False)\n",
            ")\n",
            "model.layers.16.shared_mlp.activation -> SiLUActivation()\n",
            "model.layers.16.shared_mlp.input_linear -> Linear(in_features=768, out_features=4096, bias=False)\n",
            "model.layers.16.shared_mlp.output_linear -> Linear(in_features=2048, out_features=768, bias=False)\n",
            "model.layers.16.mamba -> GraniteMoeHybridMambaLayer(\n",
            "  (act): SiLUActivation()\n",
            "  (conv1d): Conv1d(1792, 1792, kernel_size=(4,), stride=(1,), padding=(3,), groups=1792)\n",
            "  (in_proj): Linear(in_features=768, out_features=3376, bias=False)\n",
            "  (norm): GraniteMoeHybridRMSNormGated()\n",
            "  (out_proj): Linear(in_features=1536, out_features=768, bias=False)\n",
            ")\n",
            "model.layers.16.mamba.act -> SiLUActivation()\n",
            "model.layers.16.mamba.conv1d -> Conv1d(1792, 1792, kernel_size=(4,), stride=(1,), padding=(3,), groups=1792)\n",
            "model.layers.16.mamba.in_proj -> Linear(in_features=768, out_features=3376, bias=False)\n",
            "model.layers.16.mamba.norm -> GraniteMoeHybridRMSNormGated()\n",
            "model.layers.16.mamba.out_proj -> Linear(in_features=1536, out_features=768, bias=False)\n",
            "model.layers.17 -> GraniteMoeHybridDecoderLayer(\n",
            "  (input_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "  (post_attention_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "  (shared_mlp): GraniteMoeHybridMLP(\n",
            "    (activation): SiLUActivation()\n",
            "    (input_linear): Linear(in_features=768, out_features=4096, bias=False)\n",
            "    (output_linear): Linear(in_features=2048, out_features=768, bias=False)\n",
            "  )\n",
            "  (self_attn): GraniteMoeHybridAttention(\n",
            "    (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
            "    (k_proj): Linear(in_features=768, out_features=256, bias=False)\n",
            "    (v_proj): Linear(in_features=768, out_features=256, bias=False)\n",
            "    (o_proj): Linear(in_features=768, out_features=768, bias=False)\n",
            "  )\n",
            ")\n",
            "model.layers.17.input_layernorm -> GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "model.layers.17.post_attention_layernorm -> GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "model.layers.17.shared_mlp -> GraniteMoeHybridMLP(\n",
            "  (activation): SiLUActivation()\n",
            "  (input_linear): Linear(in_features=768, out_features=4096, bias=False)\n",
            "  (output_linear): Linear(in_features=2048, out_features=768, bias=False)\n",
            ")\n",
            "model.layers.17.shared_mlp.activation -> SiLUActivation()\n",
            "model.layers.17.shared_mlp.input_linear -> Linear(in_features=768, out_features=4096, bias=False)\n",
            "model.layers.17.shared_mlp.output_linear -> Linear(in_features=2048, out_features=768, bias=False)\n",
            "model.layers.17.self_attn -> GraniteMoeHybridAttention(\n",
            "  (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
            "  (k_proj): Linear(in_features=768, out_features=256, bias=False)\n",
            "  (v_proj): Linear(in_features=768, out_features=256, bias=False)\n",
            "  (o_proj): Linear(in_features=768, out_features=768, bias=False)\n",
            ")\n",
            "model.layers.17.self_attn.q_proj -> Linear(in_features=768, out_features=768, bias=False)\n",
            "model.layers.17.self_attn.k_proj -> Linear(in_features=768, out_features=256, bias=False)\n",
            "model.layers.17.self_attn.v_proj -> Linear(in_features=768, out_features=256, bias=False)\n",
            "model.layers.17.self_attn.o_proj -> Linear(in_features=768, out_features=768, bias=False)\n",
            "model.layers.18 -> GraniteMoeHybridDecoderLayer(\n",
            "  (input_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "  (post_attention_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "  (shared_mlp): GraniteMoeHybridMLP(\n",
            "    (activation): SiLUActivation()\n",
            "    (input_linear): Linear(in_features=768, out_features=4096, bias=False)\n",
            "    (output_linear): Linear(in_features=2048, out_features=768, bias=False)\n",
            "  )\n",
            "  (mamba): GraniteMoeHybridMambaLayer(\n",
            "    (act): SiLUActivation()\n",
            "    (conv1d): Conv1d(1792, 1792, kernel_size=(4,), stride=(1,), padding=(3,), groups=1792)\n",
            "    (in_proj): Linear(in_features=768, out_features=3376, bias=False)\n",
            "    (norm): GraniteMoeHybridRMSNormGated()\n",
            "    (out_proj): Linear(in_features=1536, out_features=768, bias=False)\n",
            "  )\n",
            ")\n",
            "model.layers.18.input_layernorm -> GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "model.layers.18.post_attention_layernorm -> GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "model.layers.18.shared_mlp -> GraniteMoeHybridMLP(\n",
            "  (activation): SiLUActivation()\n",
            "  (input_linear): Linear(in_features=768, out_features=4096, bias=False)\n",
            "  (output_linear): Linear(in_features=2048, out_features=768, bias=False)\n",
            ")\n",
            "model.layers.18.shared_mlp.activation -> SiLUActivation()\n",
            "model.layers.18.shared_mlp.input_linear -> Linear(in_features=768, out_features=4096, bias=False)\n",
            "model.layers.18.shared_mlp.output_linear -> Linear(in_features=2048, out_features=768, bias=False)\n",
            "model.layers.18.mamba -> GraniteMoeHybridMambaLayer(\n",
            "  (act): SiLUActivation()\n",
            "  (conv1d): Conv1d(1792, 1792, kernel_size=(4,), stride=(1,), padding=(3,), groups=1792)\n",
            "  (in_proj): Linear(in_features=768, out_features=3376, bias=False)\n",
            "  (norm): GraniteMoeHybridRMSNormGated()\n",
            "  (out_proj): Linear(in_features=1536, out_features=768, bias=False)\n",
            ")\n",
            "model.layers.18.mamba.act -> SiLUActivation()\n",
            "model.layers.18.mamba.conv1d -> Conv1d(1792, 1792, kernel_size=(4,), stride=(1,), padding=(3,), groups=1792)\n",
            "model.layers.18.mamba.in_proj -> Linear(in_features=768, out_features=3376, bias=False)\n",
            "model.layers.18.mamba.norm -> GraniteMoeHybridRMSNormGated()\n",
            "model.layers.18.mamba.out_proj -> Linear(in_features=1536, out_features=768, bias=False)\n",
            "model.layers.19 -> GraniteMoeHybridDecoderLayer(\n",
            "  (input_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "  (post_attention_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "  (shared_mlp): GraniteMoeHybridMLP(\n",
            "    (activation): SiLUActivation()\n",
            "    (input_linear): Linear(in_features=768, out_features=4096, bias=False)\n",
            "    (output_linear): Linear(in_features=2048, out_features=768, bias=False)\n",
            "  )\n",
            "  (mamba): GraniteMoeHybridMambaLayer(\n",
            "    (act): SiLUActivation()\n",
            "    (conv1d): Conv1d(1792, 1792, kernel_size=(4,), stride=(1,), padding=(3,), groups=1792)\n",
            "    (in_proj): Linear(in_features=768, out_features=3376, bias=False)\n",
            "    (norm): GraniteMoeHybridRMSNormGated()\n",
            "    (out_proj): Linear(in_features=1536, out_features=768, bias=False)\n",
            "  )\n",
            ")\n",
            "model.layers.19.input_layernorm -> GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "model.layers.19.post_attention_layernorm -> GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "model.layers.19.shared_mlp -> GraniteMoeHybridMLP(\n",
            "  (activation): SiLUActivation()\n",
            "  (input_linear): Linear(in_features=768, out_features=4096, bias=False)\n",
            "  (output_linear): Linear(in_features=2048, out_features=768, bias=False)\n",
            ")\n",
            "model.layers.19.shared_mlp.activation -> SiLUActivation()\n",
            "model.layers.19.shared_mlp.input_linear -> Linear(in_features=768, out_features=4096, bias=False)\n",
            "model.layers.19.shared_mlp.output_linear -> Linear(in_features=2048, out_features=768, bias=False)\n",
            "model.layers.19.mamba -> GraniteMoeHybridMambaLayer(\n",
            "  (act): SiLUActivation()\n",
            "  (conv1d): Conv1d(1792, 1792, kernel_size=(4,), stride=(1,), padding=(3,), groups=1792)\n",
            "  (in_proj): Linear(in_features=768, out_features=3376, bias=False)\n",
            "  (norm): GraniteMoeHybridRMSNormGated()\n",
            "  (out_proj): Linear(in_features=1536, out_features=768, bias=False)\n",
            ")\n",
            "model.layers.19.mamba.act -> SiLUActivation()\n",
            "model.layers.19.mamba.conv1d -> Conv1d(1792, 1792, kernel_size=(4,), stride=(1,), padding=(3,), groups=1792)\n",
            "model.layers.19.mamba.in_proj -> Linear(in_features=768, out_features=3376, bias=False)\n",
            "model.layers.19.mamba.norm -> GraniteMoeHybridRMSNormGated()\n",
            "model.layers.19.mamba.out_proj -> Linear(in_features=1536, out_features=768, bias=False)\n",
            "model.layers.20 -> GraniteMoeHybridDecoderLayer(\n",
            "  (input_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "  (post_attention_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "  (shared_mlp): GraniteMoeHybridMLP(\n",
            "    (activation): SiLUActivation()\n",
            "    (input_linear): Linear(in_features=768, out_features=4096, bias=False)\n",
            "    (output_linear): Linear(in_features=2048, out_features=768, bias=False)\n",
            "  )\n",
            "  (mamba): GraniteMoeHybridMambaLayer(\n",
            "    (act): SiLUActivation()\n",
            "    (conv1d): Conv1d(1792, 1792, kernel_size=(4,), stride=(1,), padding=(3,), groups=1792)\n",
            "    (in_proj): Linear(in_features=768, out_features=3376, bias=False)\n",
            "    (norm): GraniteMoeHybridRMSNormGated()\n",
            "    (out_proj): Linear(in_features=1536, out_features=768, bias=False)\n",
            "  )\n",
            ")\n",
            "model.layers.20.input_layernorm -> GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "model.layers.20.post_attention_layernorm -> GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "model.layers.20.shared_mlp -> GraniteMoeHybridMLP(\n",
            "  (activation): SiLUActivation()\n",
            "  (input_linear): Linear(in_features=768, out_features=4096, bias=False)\n",
            "  (output_linear): Linear(in_features=2048, out_features=768, bias=False)\n",
            ")\n",
            "model.layers.20.shared_mlp.activation -> SiLUActivation()\n",
            "model.layers.20.shared_mlp.input_linear -> Linear(in_features=768, out_features=4096, bias=False)\n",
            "model.layers.20.shared_mlp.output_linear -> Linear(in_features=2048, out_features=768, bias=False)\n",
            "model.layers.20.mamba -> GraniteMoeHybridMambaLayer(\n",
            "  (act): SiLUActivation()\n",
            "  (conv1d): Conv1d(1792, 1792, kernel_size=(4,), stride=(1,), padding=(3,), groups=1792)\n",
            "  (in_proj): Linear(in_features=768, out_features=3376, bias=False)\n",
            "  (norm): GraniteMoeHybridRMSNormGated()\n",
            "  (out_proj): Linear(in_features=1536, out_features=768, bias=False)\n",
            ")\n",
            "model.layers.20.mamba.act -> SiLUActivation()\n",
            "model.layers.20.mamba.conv1d -> Conv1d(1792, 1792, kernel_size=(4,), stride=(1,), padding=(3,), groups=1792)\n",
            "model.layers.20.mamba.in_proj -> Linear(in_features=768, out_features=3376, bias=False)\n",
            "model.layers.20.mamba.norm -> GraniteMoeHybridRMSNormGated()\n",
            "model.layers.20.mamba.out_proj -> Linear(in_features=1536, out_features=768, bias=False)\n",
            "model.layers.21 -> GraniteMoeHybridDecoderLayer(\n",
            "  (input_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "  (post_attention_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "  (shared_mlp): GraniteMoeHybridMLP(\n",
            "    (activation): SiLUActivation()\n",
            "    (input_linear): Linear(in_features=768, out_features=4096, bias=False)\n",
            "    (output_linear): Linear(in_features=2048, out_features=768, bias=False)\n",
            "  )\n",
            "  (mamba): GraniteMoeHybridMambaLayer(\n",
            "    (act): SiLUActivation()\n",
            "    (conv1d): Conv1d(1792, 1792, kernel_size=(4,), stride=(1,), padding=(3,), groups=1792)\n",
            "    (in_proj): Linear(in_features=768, out_features=3376, bias=False)\n",
            "    (norm): GraniteMoeHybridRMSNormGated()\n",
            "    (out_proj): Linear(in_features=1536, out_features=768, bias=False)\n",
            "  )\n",
            ")\n",
            "model.layers.21.input_layernorm -> GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "model.layers.21.post_attention_layernorm -> GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "model.layers.21.shared_mlp -> GraniteMoeHybridMLP(\n",
            "  (activation): SiLUActivation()\n",
            "  (input_linear): Linear(in_features=768, out_features=4096, bias=False)\n",
            "  (output_linear): Linear(in_features=2048, out_features=768, bias=False)\n",
            ")\n",
            "model.layers.21.shared_mlp.activation -> SiLUActivation()\n",
            "model.layers.21.shared_mlp.input_linear -> Linear(in_features=768, out_features=4096, bias=False)\n",
            "model.layers.21.shared_mlp.output_linear -> Linear(in_features=2048, out_features=768, bias=False)\n",
            "model.layers.21.mamba -> GraniteMoeHybridMambaLayer(\n",
            "  (act): SiLUActivation()\n",
            "  (conv1d): Conv1d(1792, 1792, kernel_size=(4,), stride=(1,), padding=(3,), groups=1792)\n",
            "  (in_proj): Linear(in_features=768, out_features=3376, bias=False)\n",
            "  (norm): GraniteMoeHybridRMSNormGated()\n",
            "  (out_proj): Linear(in_features=1536, out_features=768, bias=False)\n",
            ")\n",
            "model.layers.21.mamba.act -> SiLUActivation()\n",
            "model.layers.21.mamba.conv1d -> Conv1d(1792, 1792, kernel_size=(4,), stride=(1,), padding=(3,), groups=1792)\n",
            "model.layers.21.mamba.in_proj -> Linear(in_features=768, out_features=3376, bias=False)\n",
            "model.layers.21.mamba.norm -> GraniteMoeHybridRMSNormGated()\n",
            "model.layers.21.mamba.out_proj -> Linear(in_features=1536, out_features=768, bias=False)\n",
            "model.layers.22 -> GraniteMoeHybridDecoderLayer(\n",
            "  (input_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "  (post_attention_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "  (shared_mlp): GraniteMoeHybridMLP(\n",
            "    (activation): SiLUActivation()\n",
            "    (input_linear): Linear(in_features=768, out_features=4096, bias=False)\n",
            "    (output_linear): Linear(in_features=2048, out_features=768, bias=False)\n",
            "  )\n",
            "  (mamba): GraniteMoeHybridMambaLayer(\n",
            "    (act): SiLUActivation()\n",
            "    (conv1d): Conv1d(1792, 1792, kernel_size=(4,), stride=(1,), padding=(3,), groups=1792)\n",
            "    (in_proj): Linear(in_features=768, out_features=3376, bias=False)\n",
            "    (norm): GraniteMoeHybridRMSNormGated()\n",
            "    (out_proj): Linear(in_features=1536, out_features=768, bias=False)\n",
            "  )\n",
            ")\n",
            "model.layers.22.input_layernorm -> GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "model.layers.22.post_attention_layernorm -> GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "model.layers.22.shared_mlp -> GraniteMoeHybridMLP(\n",
            "  (activation): SiLUActivation()\n",
            "  (input_linear): Linear(in_features=768, out_features=4096, bias=False)\n",
            "  (output_linear): Linear(in_features=2048, out_features=768, bias=False)\n",
            ")\n",
            "model.layers.22.shared_mlp.activation -> SiLUActivation()\n",
            "model.layers.22.shared_mlp.input_linear -> Linear(in_features=768, out_features=4096, bias=False)\n",
            "model.layers.22.shared_mlp.output_linear -> Linear(in_features=2048, out_features=768, bias=False)\n",
            "model.layers.22.mamba -> GraniteMoeHybridMambaLayer(\n",
            "  (act): SiLUActivation()\n",
            "  (conv1d): Conv1d(1792, 1792, kernel_size=(4,), stride=(1,), padding=(3,), groups=1792)\n",
            "  (in_proj): Linear(in_features=768, out_features=3376, bias=False)\n",
            "  (norm): GraniteMoeHybridRMSNormGated()\n",
            "  (out_proj): Linear(in_features=1536, out_features=768, bias=False)\n",
            ")\n",
            "model.layers.22.mamba.act -> SiLUActivation()\n",
            "model.layers.22.mamba.conv1d -> Conv1d(1792, 1792, kernel_size=(4,), stride=(1,), padding=(3,), groups=1792)\n",
            "model.layers.22.mamba.in_proj -> Linear(in_features=768, out_features=3376, bias=False)\n",
            "model.layers.22.mamba.norm -> GraniteMoeHybridRMSNormGated()\n",
            "model.layers.22.mamba.out_proj -> Linear(in_features=1536, out_features=768, bias=False)\n",
            "model.layers.23 -> GraniteMoeHybridDecoderLayer(\n",
            "  (input_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "  (post_attention_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "  (shared_mlp): GraniteMoeHybridMLP(\n",
            "    (activation): SiLUActivation()\n",
            "    (input_linear): Linear(in_features=768, out_features=4096, bias=False)\n",
            "    (output_linear): Linear(in_features=2048, out_features=768, bias=False)\n",
            "  )\n",
            "  (mamba): GraniteMoeHybridMambaLayer(\n",
            "    (act): SiLUActivation()\n",
            "    (conv1d): Conv1d(1792, 1792, kernel_size=(4,), stride=(1,), padding=(3,), groups=1792)\n",
            "    (in_proj): Linear(in_features=768, out_features=3376, bias=False)\n",
            "    (norm): GraniteMoeHybridRMSNormGated()\n",
            "    (out_proj): Linear(in_features=1536, out_features=768, bias=False)\n",
            "  )\n",
            ")\n",
            "model.layers.23.input_layernorm -> GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "model.layers.23.post_attention_layernorm -> GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "model.layers.23.shared_mlp -> GraniteMoeHybridMLP(\n",
            "  (activation): SiLUActivation()\n",
            "  (input_linear): Linear(in_features=768, out_features=4096, bias=False)\n",
            "  (output_linear): Linear(in_features=2048, out_features=768, bias=False)\n",
            ")\n",
            "model.layers.23.shared_mlp.activation -> SiLUActivation()\n",
            "model.layers.23.shared_mlp.input_linear -> Linear(in_features=768, out_features=4096, bias=False)\n",
            "model.layers.23.shared_mlp.output_linear -> Linear(in_features=2048, out_features=768, bias=False)\n",
            "model.layers.23.mamba -> GraniteMoeHybridMambaLayer(\n",
            "  (act): SiLUActivation()\n",
            "  (conv1d): Conv1d(1792, 1792, kernel_size=(4,), stride=(1,), padding=(3,), groups=1792)\n",
            "  (in_proj): Linear(in_features=768, out_features=3376, bias=False)\n",
            "  (norm): GraniteMoeHybridRMSNormGated()\n",
            "  (out_proj): Linear(in_features=1536, out_features=768, bias=False)\n",
            ")\n",
            "model.layers.23.mamba.act -> SiLUActivation()\n",
            "model.layers.23.mamba.conv1d -> Conv1d(1792, 1792, kernel_size=(4,), stride=(1,), padding=(3,), groups=1792)\n",
            "model.layers.23.mamba.in_proj -> Linear(in_features=768, out_features=3376, bias=False)\n",
            "model.layers.23.mamba.norm -> GraniteMoeHybridRMSNormGated()\n",
            "model.layers.23.mamba.out_proj -> Linear(in_features=1536, out_features=768, bias=False)\n",
            "model.layers.24 -> GraniteMoeHybridDecoderLayer(\n",
            "  (input_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "  (post_attention_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "  (shared_mlp): GraniteMoeHybridMLP(\n",
            "    (activation): SiLUActivation()\n",
            "    (input_linear): Linear(in_features=768, out_features=4096, bias=False)\n",
            "    (output_linear): Linear(in_features=2048, out_features=768, bias=False)\n",
            "  )\n",
            "  (mamba): GraniteMoeHybridMambaLayer(\n",
            "    (act): SiLUActivation()\n",
            "    (conv1d): Conv1d(1792, 1792, kernel_size=(4,), stride=(1,), padding=(3,), groups=1792)\n",
            "    (in_proj): Linear(in_features=768, out_features=3376, bias=False)\n",
            "    (norm): GraniteMoeHybridRMSNormGated()\n",
            "    (out_proj): Linear(in_features=1536, out_features=768, bias=False)\n",
            "  )\n",
            ")\n",
            "model.layers.24.input_layernorm -> GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "model.layers.24.post_attention_layernorm -> GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "model.layers.24.shared_mlp -> GraniteMoeHybridMLP(\n",
            "  (activation): SiLUActivation()\n",
            "  (input_linear): Linear(in_features=768, out_features=4096, bias=False)\n",
            "  (output_linear): Linear(in_features=2048, out_features=768, bias=False)\n",
            ")\n",
            "model.layers.24.shared_mlp.activation -> SiLUActivation()\n",
            "model.layers.24.shared_mlp.input_linear -> Linear(in_features=768, out_features=4096, bias=False)\n",
            "model.layers.24.shared_mlp.output_linear -> Linear(in_features=2048, out_features=768, bias=False)\n",
            "model.layers.24.mamba -> GraniteMoeHybridMambaLayer(\n",
            "  (act): SiLUActivation()\n",
            "  (conv1d): Conv1d(1792, 1792, kernel_size=(4,), stride=(1,), padding=(3,), groups=1792)\n",
            "  (in_proj): Linear(in_features=768, out_features=3376, bias=False)\n",
            "  (norm): GraniteMoeHybridRMSNormGated()\n",
            "  (out_proj): Linear(in_features=1536, out_features=768, bias=False)\n",
            ")\n",
            "model.layers.24.mamba.act -> SiLUActivation()\n",
            "model.layers.24.mamba.conv1d -> Conv1d(1792, 1792, kernel_size=(4,), stride=(1,), padding=(3,), groups=1792)\n",
            "model.layers.24.mamba.in_proj -> Linear(in_features=768, out_features=3376, bias=False)\n",
            "model.layers.24.mamba.norm -> GraniteMoeHybridRMSNormGated()\n",
            "model.layers.24.mamba.out_proj -> Linear(in_features=1536, out_features=768, bias=False)\n",
            "model.layers.25 -> GraniteMoeHybridDecoderLayer(\n",
            "  (input_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "  (post_attention_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "  (shared_mlp): GraniteMoeHybridMLP(\n",
            "    (activation): SiLUActivation()\n",
            "    (input_linear): Linear(in_features=768, out_features=4096, bias=False)\n",
            "    (output_linear): Linear(in_features=2048, out_features=768, bias=False)\n",
            "  )\n",
            "  (mamba): GraniteMoeHybridMambaLayer(\n",
            "    (act): SiLUActivation()\n",
            "    (conv1d): Conv1d(1792, 1792, kernel_size=(4,), stride=(1,), padding=(3,), groups=1792)\n",
            "    (in_proj): Linear(in_features=768, out_features=3376, bias=False)\n",
            "    (norm): GraniteMoeHybridRMSNormGated()\n",
            "    (out_proj): Linear(in_features=1536, out_features=768, bias=False)\n",
            "  )\n",
            ")\n",
            "model.layers.25.input_layernorm -> GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "model.layers.25.post_attention_layernorm -> GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "model.layers.25.shared_mlp -> GraniteMoeHybridMLP(\n",
            "  (activation): SiLUActivation()\n",
            "  (input_linear): Linear(in_features=768, out_features=4096, bias=False)\n",
            "  (output_linear): Linear(in_features=2048, out_features=768, bias=False)\n",
            ")\n",
            "model.layers.25.shared_mlp.activation -> SiLUActivation()\n",
            "model.layers.25.shared_mlp.input_linear -> Linear(in_features=768, out_features=4096, bias=False)\n",
            "model.layers.25.shared_mlp.output_linear -> Linear(in_features=2048, out_features=768, bias=False)\n",
            "model.layers.25.mamba -> GraniteMoeHybridMambaLayer(\n",
            "  (act): SiLUActivation()\n",
            "  (conv1d): Conv1d(1792, 1792, kernel_size=(4,), stride=(1,), padding=(3,), groups=1792)\n",
            "  (in_proj): Linear(in_features=768, out_features=3376, bias=False)\n",
            "  (norm): GraniteMoeHybridRMSNormGated()\n",
            "  (out_proj): Linear(in_features=1536, out_features=768, bias=False)\n",
            ")\n",
            "model.layers.25.mamba.act -> SiLUActivation()\n",
            "model.layers.25.mamba.conv1d -> Conv1d(1792, 1792, kernel_size=(4,), stride=(1,), padding=(3,), groups=1792)\n",
            "model.layers.25.mamba.in_proj -> Linear(in_features=768, out_features=3376, bias=False)\n",
            "model.layers.25.mamba.norm -> GraniteMoeHybridRMSNormGated()\n",
            "model.layers.25.mamba.out_proj -> Linear(in_features=1536, out_features=768, bias=False)\n",
            "model.layers.26 -> GraniteMoeHybridDecoderLayer(\n",
            "  (input_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "  (post_attention_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "  (shared_mlp): GraniteMoeHybridMLP(\n",
            "    (activation): SiLUActivation()\n",
            "    (input_linear): Linear(in_features=768, out_features=4096, bias=False)\n",
            "    (output_linear): Linear(in_features=2048, out_features=768, bias=False)\n",
            "  )\n",
            "  (mamba): GraniteMoeHybridMambaLayer(\n",
            "    (act): SiLUActivation()\n",
            "    (conv1d): Conv1d(1792, 1792, kernel_size=(4,), stride=(1,), padding=(3,), groups=1792)\n",
            "    (in_proj): Linear(in_features=768, out_features=3376, bias=False)\n",
            "    (norm): GraniteMoeHybridRMSNormGated()\n",
            "    (out_proj): Linear(in_features=1536, out_features=768, bias=False)\n",
            "  )\n",
            ")\n",
            "model.layers.26.input_layernorm -> GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "model.layers.26.post_attention_layernorm -> GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "model.layers.26.shared_mlp -> GraniteMoeHybridMLP(\n",
            "  (activation): SiLUActivation()\n",
            "  (input_linear): Linear(in_features=768, out_features=4096, bias=False)\n",
            "  (output_linear): Linear(in_features=2048, out_features=768, bias=False)\n",
            ")\n",
            "model.layers.26.shared_mlp.activation -> SiLUActivation()\n",
            "model.layers.26.shared_mlp.input_linear -> Linear(in_features=768, out_features=4096, bias=False)\n",
            "model.layers.26.shared_mlp.output_linear -> Linear(in_features=2048, out_features=768, bias=False)\n",
            "model.layers.26.mamba -> GraniteMoeHybridMambaLayer(\n",
            "  (act): SiLUActivation()\n",
            "  (conv1d): Conv1d(1792, 1792, kernel_size=(4,), stride=(1,), padding=(3,), groups=1792)\n",
            "  (in_proj): Linear(in_features=768, out_features=3376, bias=False)\n",
            "  (norm): GraniteMoeHybridRMSNormGated()\n",
            "  (out_proj): Linear(in_features=1536, out_features=768, bias=False)\n",
            ")\n",
            "model.layers.26.mamba.act -> SiLUActivation()\n",
            "model.layers.26.mamba.conv1d -> Conv1d(1792, 1792, kernel_size=(4,), stride=(1,), padding=(3,), groups=1792)\n",
            "model.layers.26.mamba.in_proj -> Linear(in_features=768, out_features=3376, bias=False)\n",
            "model.layers.26.mamba.norm -> GraniteMoeHybridRMSNormGated()\n",
            "model.layers.26.mamba.out_proj -> Linear(in_features=1536, out_features=768, bias=False)\n",
            "model.layers.27 -> GraniteMoeHybridDecoderLayer(\n",
            "  (input_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "  (post_attention_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "  (shared_mlp): GraniteMoeHybridMLP(\n",
            "    (activation): SiLUActivation()\n",
            "    (input_linear): Linear(in_features=768, out_features=4096, bias=False)\n",
            "    (output_linear): Linear(in_features=2048, out_features=768, bias=False)\n",
            "  )\n",
            "  (self_attn): GraniteMoeHybridAttention(\n",
            "    (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
            "    (k_proj): Linear(in_features=768, out_features=256, bias=False)\n",
            "    (v_proj): Linear(in_features=768, out_features=256, bias=False)\n",
            "    (o_proj): Linear(in_features=768, out_features=768, bias=False)\n",
            "  )\n",
            ")\n",
            "model.layers.27.input_layernorm -> GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "model.layers.27.post_attention_layernorm -> GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "model.layers.27.shared_mlp -> GraniteMoeHybridMLP(\n",
            "  (activation): SiLUActivation()\n",
            "  (input_linear): Linear(in_features=768, out_features=4096, bias=False)\n",
            "  (output_linear): Linear(in_features=2048, out_features=768, bias=False)\n",
            ")\n",
            "model.layers.27.shared_mlp.activation -> SiLUActivation()\n",
            "model.layers.27.shared_mlp.input_linear -> Linear(in_features=768, out_features=4096, bias=False)\n",
            "model.layers.27.shared_mlp.output_linear -> Linear(in_features=2048, out_features=768, bias=False)\n",
            "model.layers.27.self_attn -> GraniteMoeHybridAttention(\n",
            "  (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
            "  (k_proj): Linear(in_features=768, out_features=256, bias=False)\n",
            "  (v_proj): Linear(in_features=768, out_features=256, bias=False)\n",
            "  (o_proj): Linear(in_features=768, out_features=768, bias=False)\n",
            ")\n",
            "model.layers.27.self_attn.q_proj -> Linear(in_features=768, out_features=768, bias=False)\n",
            "model.layers.27.self_attn.k_proj -> Linear(in_features=768, out_features=256, bias=False)\n",
            "model.layers.27.self_attn.v_proj -> Linear(in_features=768, out_features=256, bias=False)\n",
            "model.layers.27.self_attn.o_proj -> Linear(in_features=768, out_features=768, bias=False)\n",
            "model.layers.28 -> GraniteMoeHybridDecoderLayer(\n",
            "  (input_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "  (post_attention_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "  (shared_mlp): GraniteMoeHybridMLP(\n",
            "    (activation): SiLUActivation()\n",
            "    (input_linear): Linear(in_features=768, out_features=4096, bias=False)\n",
            "    (output_linear): Linear(in_features=2048, out_features=768, bias=False)\n",
            "  )\n",
            "  (mamba): GraniteMoeHybridMambaLayer(\n",
            "    (act): SiLUActivation()\n",
            "    (conv1d): Conv1d(1792, 1792, kernel_size=(4,), stride=(1,), padding=(3,), groups=1792)\n",
            "    (in_proj): Linear(in_features=768, out_features=3376, bias=False)\n",
            "    (norm): GraniteMoeHybridRMSNormGated()\n",
            "    (out_proj): Linear(in_features=1536, out_features=768, bias=False)\n",
            "  )\n",
            ")\n",
            "model.layers.28.input_layernorm -> GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "model.layers.28.post_attention_layernorm -> GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "model.layers.28.shared_mlp -> GraniteMoeHybridMLP(\n",
            "  (activation): SiLUActivation()\n",
            "  (input_linear): Linear(in_features=768, out_features=4096, bias=False)\n",
            "  (output_linear): Linear(in_features=2048, out_features=768, bias=False)\n",
            ")\n",
            "model.layers.28.shared_mlp.activation -> SiLUActivation()\n",
            "model.layers.28.shared_mlp.input_linear -> Linear(in_features=768, out_features=4096, bias=False)\n",
            "model.layers.28.shared_mlp.output_linear -> Linear(in_features=2048, out_features=768, bias=False)\n",
            "model.layers.28.mamba -> GraniteMoeHybridMambaLayer(\n",
            "  (act): SiLUActivation()\n",
            "  (conv1d): Conv1d(1792, 1792, kernel_size=(4,), stride=(1,), padding=(3,), groups=1792)\n",
            "  (in_proj): Linear(in_features=768, out_features=3376, bias=False)\n",
            "  (norm): GraniteMoeHybridRMSNormGated()\n",
            "  (out_proj): Linear(in_features=1536, out_features=768, bias=False)\n",
            ")\n",
            "model.layers.28.mamba.act -> SiLUActivation()\n",
            "model.layers.28.mamba.conv1d -> Conv1d(1792, 1792, kernel_size=(4,), stride=(1,), padding=(3,), groups=1792)\n",
            "model.layers.28.mamba.in_proj -> Linear(in_features=768, out_features=3376, bias=False)\n",
            "model.layers.28.mamba.norm -> GraniteMoeHybridRMSNormGated()\n",
            "model.layers.28.mamba.out_proj -> Linear(in_features=1536, out_features=768, bias=False)\n",
            "model.layers.29 -> GraniteMoeHybridDecoderLayer(\n",
            "  (input_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "  (post_attention_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "  (shared_mlp): GraniteMoeHybridMLP(\n",
            "    (activation): SiLUActivation()\n",
            "    (input_linear): Linear(in_features=768, out_features=4096, bias=False)\n",
            "    (output_linear): Linear(in_features=2048, out_features=768, bias=False)\n",
            "  )\n",
            "  (mamba): GraniteMoeHybridMambaLayer(\n",
            "    (act): SiLUActivation()\n",
            "    (conv1d): Conv1d(1792, 1792, kernel_size=(4,), stride=(1,), padding=(3,), groups=1792)\n",
            "    (in_proj): Linear(in_features=768, out_features=3376, bias=False)\n",
            "    (norm): GraniteMoeHybridRMSNormGated()\n",
            "    (out_proj): Linear(in_features=1536, out_features=768, bias=False)\n",
            "  )\n",
            ")\n",
            "model.layers.29.input_layernorm -> GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "model.layers.29.post_attention_layernorm -> GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "model.layers.29.shared_mlp -> GraniteMoeHybridMLP(\n",
            "  (activation): SiLUActivation()\n",
            "  (input_linear): Linear(in_features=768, out_features=4096, bias=False)\n",
            "  (output_linear): Linear(in_features=2048, out_features=768, bias=False)\n",
            ")\n",
            "model.layers.29.shared_mlp.activation -> SiLUActivation()\n",
            "model.layers.29.shared_mlp.input_linear -> Linear(in_features=768, out_features=4096, bias=False)\n",
            "model.layers.29.shared_mlp.output_linear -> Linear(in_features=2048, out_features=768, bias=False)\n",
            "model.layers.29.mamba -> GraniteMoeHybridMambaLayer(\n",
            "  (act): SiLUActivation()\n",
            "  (conv1d): Conv1d(1792, 1792, kernel_size=(4,), stride=(1,), padding=(3,), groups=1792)\n",
            "  (in_proj): Linear(in_features=768, out_features=3376, bias=False)\n",
            "  (norm): GraniteMoeHybridRMSNormGated()\n",
            "  (out_proj): Linear(in_features=1536, out_features=768, bias=False)\n",
            ")\n",
            "model.layers.29.mamba.act -> SiLUActivation()\n",
            "model.layers.29.mamba.conv1d -> Conv1d(1792, 1792, kernel_size=(4,), stride=(1,), padding=(3,), groups=1792)\n",
            "model.layers.29.mamba.in_proj -> Linear(in_features=768, out_features=3376, bias=False)\n",
            "model.layers.29.mamba.norm -> GraniteMoeHybridRMSNormGated()\n",
            "model.layers.29.mamba.out_proj -> Linear(in_features=1536, out_features=768, bias=False)\n",
            "model.layers.30 -> GraniteMoeHybridDecoderLayer(\n",
            "  (input_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "  (post_attention_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "  (shared_mlp): GraniteMoeHybridMLP(\n",
            "    (activation): SiLUActivation()\n",
            "    (input_linear): Linear(in_features=768, out_features=4096, bias=False)\n",
            "    (output_linear): Linear(in_features=2048, out_features=768, bias=False)\n",
            "  )\n",
            "  (mamba): GraniteMoeHybridMambaLayer(\n",
            "    (act): SiLUActivation()\n",
            "    (conv1d): Conv1d(1792, 1792, kernel_size=(4,), stride=(1,), padding=(3,), groups=1792)\n",
            "    (in_proj): Linear(in_features=768, out_features=3376, bias=False)\n",
            "    (norm): GraniteMoeHybridRMSNormGated()\n",
            "    (out_proj): Linear(in_features=1536, out_features=768, bias=False)\n",
            "  )\n",
            ")\n",
            "model.layers.30.input_layernorm -> GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "model.layers.30.post_attention_layernorm -> GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "model.layers.30.shared_mlp -> GraniteMoeHybridMLP(\n",
            "  (activation): SiLUActivation()\n",
            "  (input_linear): Linear(in_features=768, out_features=4096, bias=False)\n",
            "  (output_linear): Linear(in_features=2048, out_features=768, bias=False)\n",
            ")\n",
            "model.layers.30.shared_mlp.activation -> SiLUActivation()\n",
            "model.layers.30.shared_mlp.input_linear -> Linear(in_features=768, out_features=4096, bias=False)\n",
            "model.layers.30.shared_mlp.output_linear -> Linear(in_features=2048, out_features=768, bias=False)\n",
            "model.layers.30.mamba -> GraniteMoeHybridMambaLayer(\n",
            "  (act): SiLUActivation()\n",
            "  (conv1d): Conv1d(1792, 1792, kernel_size=(4,), stride=(1,), padding=(3,), groups=1792)\n",
            "  (in_proj): Linear(in_features=768, out_features=3376, bias=False)\n",
            "  (norm): GraniteMoeHybridRMSNormGated()\n",
            "  (out_proj): Linear(in_features=1536, out_features=768, bias=False)\n",
            ")\n",
            "model.layers.30.mamba.act -> SiLUActivation()\n",
            "model.layers.30.mamba.conv1d -> Conv1d(1792, 1792, kernel_size=(4,), stride=(1,), padding=(3,), groups=1792)\n",
            "model.layers.30.mamba.in_proj -> Linear(in_features=768, out_features=3376, bias=False)\n",
            "model.layers.30.mamba.norm -> GraniteMoeHybridRMSNormGated()\n",
            "model.layers.30.mamba.out_proj -> Linear(in_features=1536, out_features=768, bias=False)\n",
            "model.layers.31 -> GraniteMoeHybridDecoderLayer(\n",
            "  (input_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "  (post_attention_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "  (shared_mlp): GraniteMoeHybridMLP(\n",
            "    (activation): SiLUActivation()\n",
            "    (input_linear): Linear(in_features=768, out_features=4096, bias=False)\n",
            "    (output_linear): Linear(in_features=2048, out_features=768, bias=False)\n",
            "  )\n",
            "  (mamba): GraniteMoeHybridMambaLayer(\n",
            "    (act): SiLUActivation()\n",
            "    (conv1d): Conv1d(1792, 1792, kernel_size=(4,), stride=(1,), padding=(3,), groups=1792)\n",
            "    (in_proj): Linear(in_features=768, out_features=3376, bias=False)\n",
            "    (norm): GraniteMoeHybridRMSNormGated()\n",
            "    (out_proj): Linear(in_features=1536, out_features=768, bias=False)\n",
            "  )\n",
            ")\n",
            "model.layers.31.input_layernorm -> GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "model.layers.31.post_attention_layernorm -> GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "model.layers.31.shared_mlp -> GraniteMoeHybridMLP(\n",
            "  (activation): SiLUActivation()\n",
            "  (input_linear): Linear(in_features=768, out_features=4096, bias=False)\n",
            "  (output_linear): Linear(in_features=2048, out_features=768, bias=False)\n",
            ")\n",
            "model.layers.31.shared_mlp.activation -> SiLUActivation()\n",
            "model.layers.31.shared_mlp.input_linear -> Linear(in_features=768, out_features=4096, bias=False)\n",
            "model.layers.31.shared_mlp.output_linear -> Linear(in_features=2048, out_features=768, bias=False)\n",
            "model.layers.31.mamba -> GraniteMoeHybridMambaLayer(\n",
            "  (act): SiLUActivation()\n",
            "  (conv1d): Conv1d(1792, 1792, kernel_size=(4,), stride=(1,), padding=(3,), groups=1792)\n",
            "  (in_proj): Linear(in_features=768, out_features=3376, bias=False)\n",
            "  (norm): GraniteMoeHybridRMSNormGated()\n",
            "  (out_proj): Linear(in_features=1536, out_features=768, bias=False)\n",
            ")\n",
            "model.layers.31.mamba.act -> SiLUActivation()\n",
            "model.layers.31.mamba.conv1d -> Conv1d(1792, 1792, kernel_size=(4,), stride=(1,), padding=(3,), groups=1792)\n",
            "model.layers.31.mamba.in_proj -> Linear(in_features=768, out_features=3376, bias=False)\n",
            "model.layers.31.mamba.norm -> GraniteMoeHybridRMSNormGated()\n",
            "model.layers.31.mamba.out_proj -> Linear(in_features=1536, out_features=768, bias=False)\n",
            "model.norm -> GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "lm_head -> Linear(in_features=768, out_features=100352, bias=False)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for name, module in model.named_children():\n",
        "    print(name, module)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "JfCPgj63o51x",
        "outputId": "132eef20-33e2-4e2b-ac89-f3360f79e70b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model GraniteMoeHybridModel(\n",
            "  (embed_tokens): Embedding(100352, 768, padding_idx=100256)\n",
            "  (layers): ModuleList(\n",
            "    (0-9): 10 x GraniteMoeHybridDecoderLayer(\n",
            "      (input_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "      (post_attention_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "      (shared_mlp): GraniteMoeHybridMLP(\n",
            "        (activation): SiLUActivation()\n",
            "        (input_linear): Linear(in_features=768, out_features=4096, bias=False)\n",
            "        (output_linear): Linear(in_features=2048, out_features=768, bias=False)\n",
            "      )\n",
            "      (mamba): GraniteMoeHybridMambaLayer(\n",
            "        (act): SiLUActivation()\n",
            "        (conv1d): Conv1d(1792, 1792, kernel_size=(4,), stride=(1,), padding=(3,), groups=1792)\n",
            "        (in_proj): Linear(in_features=768, out_features=3376, bias=False)\n",
            "        (norm): GraniteMoeHybridRMSNormGated()\n",
            "        (out_proj): Linear(in_features=1536, out_features=768, bias=False)\n",
            "      )\n",
            "    )\n",
            "    (10): GraniteMoeHybridDecoderLayer(\n",
            "      (input_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "      (post_attention_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "      (shared_mlp): GraniteMoeHybridMLP(\n",
            "        (activation): SiLUActivation()\n",
            "        (input_linear): Linear(in_features=768, out_features=4096, bias=False)\n",
            "        (output_linear): Linear(in_features=2048, out_features=768, bias=False)\n",
            "      )\n",
            "      (self_attn): GraniteMoeHybridAttention(\n",
            "        (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (k_proj): Linear(in_features=768, out_features=256, bias=False)\n",
            "        (v_proj): Linear(in_features=768, out_features=256, bias=False)\n",
            "        (o_proj): Linear(in_features=768, out_features=768, bias=False)\n",
            "      )\n",
            "    )\n",
            "    (11-12): 2 x GraniteMoeHybridDecoderLayer(\n",
            "      (input_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "      (post_attention_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "      (shared_mlp): GraniteMoeHybridMLP(\n",
            "        (activation): SiLUActivation()\n",
            "        (input_linear): Linear(in_features=768, out_features=4096, bias=False)\n",
            "        (output_linear): Linear(in_features=2048, out_features=768, bias=False)\n",
            "      )\n",
            "      (mamba): GraniteMoeHybridMambaLayer(\n",
            "        (act): SiLUActivation()\n",
            "        (conv1d): Conv1d(1792, 1792, kernel_size=(4,), stride=(1,), padding=(3,), groups=1792)\n",
            "        (in_proj): Linear(in_features=768, out_features=3376, bias=False)\n",
            "        (norm): GraniteMoeHybridRMSNormGated()\n",
            "        (out_proj): Linear(in_features=1536, out_features=768, bias=False)\n",
            "      )\n",
            "    )\n",
            "    (13): GraniteMoeHybridDecoderLayer(\n",
            "      (input_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "      (post_attention_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "      (shared_mlp): GraniteMoeHybridMLP(\n",
            "        (activation): SiLUActivation()\n",
            "        (input_linear): Linear(in_features=768, out_features=4096, bias=False)\n",
            "        (output_linear): Linear(in_features=2048, out_features=768, bias=False)\n",
            "      )\n",
            "      (self_attn): GraniteMoeHybridAttention(\n",
            "        (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (k_proj): Linear(in_features=768, out_features=256, bias=False)\n",
            "        (v_proj): Linear(in_features=768, out_features=256, bias=False)\n",
            "        (o_proj): Linear(in_features=768, out_features=768, bias=False)\n",
            "      )\n",
            "    )\n",
            "    (14-16): 3 x GraniteMoeHybridDecoderLayer(\n",
            "      (input_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "      (post_attention_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "      (shared_mlp): GraniteMoeHybridMLP(\n",
            "        (activation): SiLUActivation()\n",
            "        (input_linear): Linear(in_features=768, out_features=4096, bias=False)\n",
            "        (output_linear): Linear(in_features=2048, out_features=768, bias=False)\n",
            "      )\n",
            "      (mamba): GraniteMoeHybridMambaLayer(\n",
            "        (act): SiLUActivation()\n",
            "        (conv1d): Conv1d(1792, 1792, kernel_size=(4,), stride=(1,), padding=(3,), groups=1792)\n",
            "        (in_proj): Linear(in_features=768, out_features=3376, bias=False)\n",
            "        (norm): GraniteMoeHybridRMSNormGated()\n",
            "        (out_proj): Linear(in_features=1536, out_features=768, bias=False)\n",
            "      )\n",
            "    )\n",
            "    (17): GraniteMoeHybridDecoderLayer(\n",
            "      (input_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "      (post_attention_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "      (shared_mlp): GraniteMoeHybridMLP(\n",
            "        (activation): SiLUActivation()\n",
            "        (input_linear): Linear(in_features=768, out_features=4096, bias=False)\n",
            "        (output_linear): Linear(in_features=2048, out_features=768, bias=False)\n",
            "      )\n",
            "      (self_attn): GraniteMoeHybridAttention(\n",
            "        (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (k_proj): Linear(in_features=768, out_features=256, bias=False)\n",
            "        (v_proj): Linear(in_features=768, out_features=256, bias=False)\n",
            "        (o_proj): Linear(in_features=768, out_features=768, bias=False)\n",
            "      )\n",
            "    )\n",
            "    (18-26): 9 x GraniteMoeHybridDecoderLayer(\n",
            "      (input_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "      (post_attention_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "      (shared_mlp): GraniteMoeHybridMLP(\n",
            "        (activation): SiLUActivation()\n",
            "        (input_linear): Linear(in_features=768, out_features=4096, bias=False)\n",
            "        (output_linear): Linear(in_features=2048, out_features=768, bias=False)\n",
            "      )\n",
            "      (mamba): GraniteMoeHybridMambaLayer(\n",
            "        (act): SiLUActivation()\n",
            "        (conv1d): Conv1d(1792, 1792, kernel_size=(4,), stride=(1,), padding=(3,), groups=1792)\n",
            "        (in_proj): Linear(in_features=768, out_features=3376, bias=False)\n",
            "        (norm): GraniteMoeHybridRMSNormGated()\n",
            "        (out_proj): Linear(in_features=1536, out_features=768, bias=False)\n",
            "      )\n",
            "    )\n",
            "    (27): GraniteMoeHybridDecoderLayer(\n",
            "      (input_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "      (post_attention_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "      (shared_mlp): GraniteMoeHybridMLP(\n",
            "        (activation): SiLUActivation()\n",
            "        (input_linear): Linear(in_features=768, out_features=4096, bias=False)\n",
            "        (output_linear): Linear(in_features=2048, out_features=768, bias=False)\n",
            "      )\n",
            "      (self_attn): GraniteMoeHybridAttention(\n",
            "        (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
            "        (k_proj): Linear(in_features=768, out_features=256, bias=False)\n",
            "        (v_proj): Linear(in_features=768, out_features=256, bias=False)\n",
            "        (o_proj): Linear(in_features=768, out_features=768, bias=False)\n",
            "      )\n",
            "    )\n",
            "    (28-31): 4 x GraniteMoeHybridDecoderLayer(\n",
            "      (input_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "      (post_attention_layernorm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            "      (shared_mlp): GraniteMoeHybridMLP(\n",
            "        (activation): SiLUActivation()\n",
            "        (input_linear): Linear(in_features=768, out_features=4096, bias=False)\n",
            "        (output_linear): Linear(in_features=2048, out_features=768, bias=False)\n",
            "      )\n",
            "      (mamba): GraniteMoeHybridMambaLayer(\n",
            "        (act): SiLUActivation()\n",
            "        (conv1d): Conv1d(1792, 1792, kernel_size=(4,), stride=(1,), padding=(3,), groups=1792)\n",
            "        (in_proj): Linear(in_features=768, out_features=3376, bias=False)\n",
            "        (norm): GraniteMoeHybridRMSNormGated()\n",
            "        (out_proj): Linear(in_features=1536, out_features=768, bias=False)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (norm): GraniteMoeHybridRMSNorm((768,), eps=1e-05)\n",
            ")\n",
            "lm_head Linear(in_features=768, out_features=100352, bias=False)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Method 3 â€” Use a â€œmodel summaryâ€ tool to get a tabular overview"
      ],
      "metadata": {
        "id": "I6s3fDb6rBGb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchinfo import summary\n",
        "\n",
        "# For example (you might need to adjust input shape / dtype properly):\n",
        "summary(model)  # e.g. batch_size=1, seq_length=10\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "JW3BVTZUpK10",
        "outputId": "6e9bb5be-9add-4076-ce10-983371e2496e"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                                            Param #\n",
              "==========================================================================================\n",
              "GraniteMoeHybridForCausalLM                                       --\n",
              "â”œâ”€GraniteMoeHybridModel: 1-1                                      --\n",
              "â”‚    â””â”€Embedding: 2-1                                             77,070,336\n",
              "â”‚    â””â”€ModuleList: 2-2                                            --\n",
              "â”‚    â”‚    â””â”€GraniteMoeHybridDecoderLayer: 3-1                     8,503,184\n",
              "â”‚    â”‚    â””â”€GraniteMoeHybridDecoderLayer: 3-2                     8,503,184\n",
              "â”‚    â”‚    â””â”€GraniteMoeHybridDecoderLayer: 3-3                     8,503,184\n",
              "â”‚    â”‚    â””â”€GraniteMoeHybridDecoderLayer: 3-4                     8,503,184\n",
              "â”‚    â”‚    â””â”€GraniteMoeHybridDecoderLayer: 3-5                     8,503,184\n",
              "â”‚    â”‚    â””â”€GraniteMoeHybridDecoderLayer: 3-6                     8,503,184\n",
              "â”‚    â”‚    â””â”€GraniteMoeHybridDecoderLayer: 3-7                     8,503,184\n",
              "â”‚    â”‚    â””â”€GraniteMoeHybridDecoderLayer: 3-8                     8,503,184\n",
              "â”‚    â”‚    â””â”€GraniteMoeHybridDecoderLayer: 3-9                     8,503,184\n",
              "â”‚    â”‚    â””â”€GraniteMoeHybridDecoderLayer: 3-10                    8,503,184\n",
              "â”‚    â”‚    â””â”€GraniteMoeHybridDecoderLayer: 3-11                    6,292,992\n",
              "â”‚    â”‚    â””â”€GraniteMoeHybridDecoderLayer: 3-12                    8,503,184\n",
              "â”‚    â”‚    â””â”€GraniteMoeHybridDecoderLayer: 3-13                    8,503,184\n",
              "â”‚    â”‚    â””â”€GraniteMoeHybridDecoderLayer: 3-14                    6,292,992\n",
              "â”‚    â”‚    â””â”€GraniteMoeHybridDecoderLayer: 3-15                    8,503,184\n",
              "â”‚    â”‚    â””â”€GraniteMoeHybridDecoderLayer: 3-16                    8,503,184\n",
              "â”‚    â”‚    â””â”€GraniteMoeHybridDecoderLayer: 3-17                    8,503,184\n",
              "â”‚    â”‚    â””â”€GraniteMoeHybridDecoderLayer: 3-18                    6,292,992\n",
              "â”‚    â”‚    â””â”€GraniteMoeHybridDecoderLayer: 3-19                    8,503,184\n",
              "â”‚    â”‚    â””â”€GraniteMoeHybridDecoderLayer: 3-20                    8,503,184\n",
              "â”‚    â”‚    â””â”€GraniteMoeHybridDecoderLayer: 3-21                    8,503,184\n",
              "â”‚    â”‚    â””â”€GraniteMoeHybridDecoderLayer: 3-22                    8,503,184\n",
              "â”‚    â”‚    â””â”€GraniteMoeHybridDecoderLayer: 3-23                    8,503,184\n",
              "â”‚    â”‚    â””â”€GraniteMoeHybridDecoderLayer: 3-24                    8,503,184\n",
              "â”‚    â”‚    â””â”€GraniteMoeHybridDecoderLayer: 3-25                    8,503,184\n",
              "â”‚    â”‚    â””â”€GraniteMoeHybridDecoderLayer: 3-26                    8,503,184\n",
              "â”‚    â”‚    â””â”€GraniteMoeHybridDecoderLayer: 3-27                    8,503,184\n",
              "â”‚    â”‚    â””â”€GraniteMoeHybridDecoderLayer: 3-28                    6,292,992\n",
              "â”‚    â”‚    â””â”€GraniteMoeHybridDecoderLayer: 3-29                    8,503,184\n",
              "â”‚    â”‚    â””â”€GraniteMoeHybridDecoderLayer: 3-30                    8,503,184\n",
              "â”‚    â”‚    â””â”€GraniteMoeHybridDecoderLayer: 3-31                    8,503,184\n",
              "â”‚    â”‚    â””â”€GraniteMoeHybridDecoderLayer: 3-32                    8,503,184\n",
              "â”‚    â””â”€GraniteMoeHybridRMSNorm: 2-3                               768\n",
              "â”œâ”€Linear: 1-2                                                     77,070,336\n",
              "==========================================================================================\n",
              "Total params: 417,402,560\n",
              "Trainable params: 417,402,560\n",
              "Non-trainable params: 0\n",
              "=========================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tip"
      ],
      "metadata": {
        "id": "zmINfx6_rHwe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = model.config\n",
        "print(config)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "kZCw1z_iqOgw",
        "outputId": "ac14d42a-d70a-4a29-ec63-1553eb256ee9"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GraniteMoeHybridConfig {\n",
            "  \"architectures\": [\n",
            "    \"GraniteMoeHybridForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"attention_multiplier\": 0.015625,\n",
            "  \"bos_token_id\": 100257,\n",
            "  \"dtype\": \"float32\",\n",
            "  \"embedding_multiplier\": 12,\n",
            "  \"eos_token_id\": 100257,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 768,\n",
            "  \"init_method\": \"mup\",\n",
            "  \"initializer_range\": 0.1,\n",
            "  \"intermediate_size\": 2048,\n",
            "  \"layer_types\": [\n",
            "    \"mamba\",\n",
            "    \"mamba\",\n",
            "    \"mamba\",\n",
            "    \"mamba\",\n",
            "    \"mamba\",\n",
            "    \"mamba\",\n",
            "    \"mamba\",\n",
            "    \"mamba\",\n",
            "    \"mamba\",\n",
            "    \"mamba\",\n",
            "    \"attention\",\n",
            "    \"mamba\",\n",
            "    \"mamba\",\n",
            "    \"attention\",\n",
            "    \"mamba\",\n",
            "    \"mamba\",\n",
            "    \"mamba\",\n",
            "    \"attention\",\n",
            "    \"mamba\",\n",
            "    \"mamba\",\n",
            "    \"mamba\",\n",
            "    \"mamba\",\n",
            "    \"mamba\",\n",
            "    \"mamba\",\n",
            "    \"mamba\",\n",
            "    \"mamba\",\n",
            "    \"mamba\",\n",
            "    \"attention\",\n",
            "    \"mamba\",\n",
            "    \"mamba\",\n",
            "    \"mamba\",\n",
            "    \"mamba\"\n",
            "  ],\n",
            "  \"logits_scaling\": 3,\n",
            "  \"mamba_chunk_size\": 256,\n",
            "  \"mamba_conv_bias\": true,\n",
            "  \"mamba_d_conv\": 4,\n",
            "  \"mamba_d_head\": 32,\n",
            "  \"mamba_d_state\": 128,\n",
            "  \"mamba_expand\": 2,\n",
            "  \"mamba_n_groups\": 1,\n",
            "  \"mamba_n_heads\": 48,\n",
            "  \"mamba_proj_bias\": false,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"model_type\": \"granitemoehybrid\",\n",
            "  \"normalization_function\": \"rmsnorm\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_experts_per_tok\": 0,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 4,\n",
            "  \"num_local_experts\": 0,\n",
            "  \"output_router_logits\": false,\n",
            "  \"pad_token_id\": 100256,\n",
            "  \"position_embedding_type\": \"nope\",\n",
            "  \"residual_multiplier\": 0.246,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000,\n",
            "  \"router_aux_loss_coef\": 0.01,\n",
            "  \"shared_intermediate_size\": 2048,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"transformers_version\": \"4.57.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 100352\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TShndTmLqRCr"
      },
      "execution_count": 21,
      "outputs": []
    }
  ]
}