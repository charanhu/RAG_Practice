{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eC8oCKZTWR2u",
        "outputId": "205677e8-f96c-4d3c-9b23-77691f1e6d0f"
      },
      "outputs": [],
      "source": [
        "! pip install -q langchain langchain-groq python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "oWbS6BlwksU5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "up-CBZJ9k0bK"
      },
      "outputs": [],
      "source": [
        "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Mdf-FPxhlH_e"
      },
      "outputs": [],
      "source": [
        "from langchain.agents import create_agent\n",
        "\n",
        "\n",
        "agent = create_agent(\n",
        "    model=\"groq:llama-3.1-8b-instant\",\n",
        "    tools=[],\n",
        "    system_prompt=\"You are a helpful chat assistant. Be clear, concise, and polite. Understand the userâ€™s intent and respond directly. Stay professional and safe.\"\n",
        ")\n",
        "\n",
        "\n",
        "result = agent.invoke(\n",
        "    {\n",
        "        \"messages\": [{\"role\": \"user\", \"content\": \"Explain Machine Learning in short\"}]\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nB7o0MgImeY2",
        "outputId": "a597e4c5-4b2c-425a-e304-f54dd1d20527"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'messages': [HumanMessage(content='Explain Machine Learning in short', additional_kwargs={}, response_metadata={}, id='0c7076a7-46b9-4588-9673-0b1c17a99594'), AIMessage(content=\"**Machine Learning (ML) in a nutshell:**\\n\\nMachine Learning is a subset of Artificial Intelligence (AI) that enables computers to learn from data without being explicitly programmed. It's a process where algorithms are trained on data to make predictions, classify objects, or make decisions.\\n\\n**Key components:**\\n\\n1. **Training data**: The data used to train the algorithm.\\n2. **Algorithm**: The set of rules and models used to analyze the data.\\n3. **Model**: The trained algorithm that makes predictions or decisions.\\n\\n**Types of Machine Learning:**\\n\\n1. **Supervised Learning**: Algorithm is trained on labeled data to predict outcomes.\\n2. **Unsupervised Learning**: Algorithm is trained on unlabeled data to identify patterns.\\n3. **Reinforcement Learning**: Algorithm learns from trial and error to make decisions.\\n\\nMachine Learning has numerous applications in image recognition, natural language processing, speech recognition, and more.\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 186, 'prompt_tokens': 70, 'total_tokens': 256, 'completion_time': 0.260401706, 'completion_tokens_details': None, 'prompt_time': 0.00508632, 'prompt_tokens_details': None, 'queue_time': 0.054264509, 'total_time': 0.265488026}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_f757f4b0bf', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--beb82119-0d5d-42de-9deb-2852ba9dce3e-0', usage_metadata={'input_tokens': 70, 'output_tokens': 186, 'total_tokens': 256})]}\n"
          ]
        }
      ],
      "source": [
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CIswHDn_mhki",
        "outputId": "875b7475-183b-42af-b37c-66eb57484789"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "**Machine Learning (ML) in a nutshell:**\n",
            "\n",
            "Machine Learning is a subset of Artificial Intelligence (AI) that enables computers to learn from data without being explicitly programmed. It's a process where algorithms are trained on data to make predictions, classify objects, or make decisions.\n",
            "\n",
            "**Key components:**\n",
            "\n",
            "1. **Training data**: The data used to train the algorithm.\n",
            "2. **Algorithm**: The set of rules and models used to analyze the data.\n",
            "3. **Model**: The trained algorithm that makes predictions or decisions.\n",
            "\n",
            "**Types of Machine Learning:**\n",
            "\n",
            "1. **Supervised Learning**: Algorithm is trained on labeled data to predict outcomes.\n",
            "2. **Unsupervised Learning**: Algorithm is trained on unlabeled data to identify patterns.\n",
            "3. **Reinforcement Learning**: Algorithm learns from trial and error to make decisions.\n",
            "\n",
            "Machine Learning has numerous applications in image recognition, natural language processing, speech recognition, and more.\n"
          ]
        }
      ],
      "source": [
        "print(result[\"messages\"][1].content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "prG6EM7em8Zz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a72X4mTtnX7-",
        "outputId": "76b2a17c-a89e-4365-b8fe-473d0b6f6f52"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ¤– Chat Assistant ready! Type 'bye' or 'exit' to stop.\n",
            "\n",
            "\n",
            "\n",
            "Assistant: Goodbye ðŸ‘‹\n"
          ]
        }
      ],
      "source": [
        "# langchain v1 + Groq + create_agent + simple cli chat with chat history\n",
        "\n",
        "import os\n",
        "from langchain.agents import create_agent\n",
        "\n",
        "os.environ[\"GROQ_API_KEY\"] = \"your_groq_api_key\"\n",
        "\n",
        "# configure the LLM\n",
        "model = \"groq:llama-3.1-8b-instant\"\n",
        "agent = create_agent(\n",
        "    model=model,\n",
        "    tools=[],\n",
        "    system_prompt=(\n",
        "        \"You are a helpful chat assistant. Be clear, concise, and polite. \"\n",
        "        \"Understand the userâ€™s intent and respond directly. Stay professional and safe.\"\n",
        "    ),\n",
        ")\n",
        "\n",
        "# chat history list\n",
        "chat_history = []\n",
        "\n",
        "\n",
        "print(\"ðŸ¤– Chat Assistant ready! Type 'bye' or 'exit' to stop.\\n\")\n",
        "\n",
        "# simple chat loop\n",
        "while True:\n",
        "  user_input = input(\"You: \".strip())\n",
        "  print(\"\\n\")\n",
        "  if user_input.lower() in [\"bye\", \"exit\"]:\n",
        "    print(\"Assistant: Goodbye ðŸ‘‹\")\n",
        "    break\n",
        "\n",
        "  messages = chat_history + [{\"role\": \"user\", \"content\": user_input}]\n",
        "  result = agent.invoke({\"messages\": messages})\n",
        "\n",
        "  # extract reply\n",
        "  try:\n",
        "    reply = result[\"messages\"][-1].content\n",
        "  except Exception as e:\n",
        "    reply = str(e)\n",
        "\n",
        "  print(f\"Asistant: {reply}\\n\")\n",
        "  print(\"-\"*60)\n",
        "\n",
        "  # update chat history\n",
        "  chat_history.append({\"role\": \"user\", \"content\": user_input})\n",
        "  chat_history.append({\"role\": \"assistant\", \"content\": reply})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CehuRdekp_MW"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
